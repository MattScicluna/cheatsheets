\documentclass[]{article}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathreplacing}
\usepackage[hidelinks]{hyperref}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{xcolor} % For color
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
%\usepackage{enumerate} % For lettered enumeration
\usepackage{amsthm}

%  Make theorems look nice
\newtheoremstyle{mattstyle}%                % Name
{}%                                     % Space above
{}%                                     % Space below
{\itshape}%                                     % Body font
{}%                                     % Indent amount
{\bfseries}%                            % Theorem head font
{.}%                                    % Punctuation after theorem head
{ }%                                    % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%      

%  State and prove theorems
\theoremstyle{mattstyle}
\newtheorem{theorem}{Theorem}[section]

%  Definitions
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{document}

\title{Machine Learning Notes}
\author{Matthew Scicluna}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Statistics and Probability Background}

We discuss the statistical background of Machine Learning and introduce the ideas that will be used throughout these notes.

\subsection{Empirical Distribution}

Given some data \(x_1, \cdots x_n \sim F\) where $F$ is an unknown CDF, we want to approximate this using some mapping $\hat{F}$ called the \textbf{Empirical Distribution} of the data.

\begin{equation}
	\hat{F}(t) = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{x_i \le t\}}
\end{equation} 

It can be shown that $\hat{F}(t) \rightarrow F(t) \ a.s.\ \forall t$, justifying its use as an approximation of $F$, provided enough data has been observed. As with $F$ we can approximate $f$. We define the \textbf{Empirical Density Function} $\hat{f}$:

\begin{equation}
	\hat{f}(t) = \frac{1}{n}\sum_{i=1}^n \delta(x_i, t)
\end{equation}

Where $\delta$ is defined differently in the continuous and discrete case. In the continuous case it is called the \textbf{Dirac Delta Function}:
	\begin{equation}
		\delta(x,y) = \begin{cases}
		\infty & x=y \\
		0 & \text{o.w.}
	\end{cases}
	\end{equation}
	Additionally, we suppose that:
	\begin{enumerate}
		\item $\int_{-\infty}^{\infty} \delta(t,y)dt = 1$
		\item $\int \delta(t,y) f(t) dt = f(y)$, for any $f$ with compact support that is continuous around $y$
	\end{enumerate}

This is not a function, but is called a \emph{Generalized Function}. In the discrete case things are much simpler, as we can use the simpler \textbf{Kronecker delta function}:
\begin{equation}
	\delta(x,y) = 
	\begin{cases}
		1 & x=y \\
		0 & \text{o.w.}
	\end{cases}
\end{equation}

Finally, we notice that $\hat{f}$ and $\hat{F}$ satisfy an important relationship that would be expected from the cdf and pdf: $\int_{-\infty}^{t}\hat{f}(y)dy = \hat{F}(t)$.

\begin{align*}
\int_{-\infty}^{t}\hat{f}(y)dy &= \int_{-\infty}^{t} \frac{1}{n}\sum_{i=1}^n \delta(x_i, y)dy\\
&= \frac{1}{n}\sum_{i=1}^n \int_{-\infty}^{t}  \delta(x_i, y)dy\\
&= \frac{1}{n}\sum_{i=1}^n \int_{-\infty}^{\infty} \mathbbm{1}_{\{x_i \le y\}} \delta(x_i, y)dy\\
&= \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{x_i \le y\}}\\
&= \hat{F}(t)
\end{align*}

\subsection{Bayesian Statistics}

Given an Event, what does the probability of that Event \emph{mean}? There are two interpretations:

\begin{enumerate}
	\item \textbf{Frequentists}: the \emph{limiting frequency} of the event
	\item \textbf{Bayesians}: the \emph{reasonable expectation} that the event occurs
\end{enumerate}

The \emph{reasonable expectation} can be further broken down into two views. The \textbf{Objective Bayesians} view the \emph{reasonable expectation} as the \emph{state of knowledge}. They view probability as an extension of propositional logic, which is described in \cite{jaynes}. The \textbf{Subjective Bayesians} view probability as a quantification of \emph{personal belief}. The main difference between the groups is in how they choose their priors: the Subjective Bayesians use knowledge about or prior experience with model parameters, wheras the Objectivists try to introduce as little prior knowledge as possible, using noninformative priors.

\subsubsection{Existence Of The Prior}
We need a theoretical justification for why we assume the existence of a prior distribution on \(\theta\) in the first place! The justification for this requires the \textbf{Infinite Exchangeable} assumption of the data \(\{x_i\}_{i=1}^{\infty}\). This is satisfied when, given a sequence of random variables, any finite subset \(\{x_j\}_{j=1}^{n}\), and any permutation of this subset \(\pi_{1:n}\) 
\begin{equation}
p(x_1, \cdots, x_n) = p(x_{\pi_1}, \cdots, x_{\pi_n})
\end{equation}

It turns out the above is equivalent to assuming the existence of the prior! The following theorem makes this precise.

\begin{theorem}[De Finetti Theorem]
	A sequence is Infinite Exchangeable iff for any \(n\)
	$$ p(x_1, \cdots x_n) = \int\prod_{i=1}^n p(x_i|\theta)P(d\theta) $$
	for some measure \(P\) on \(\theta\). Also, if $\theta$ has a density then $P(d\theta) = p(\theta)d\theta$. Note: \(\theta\) may be infinite!
\end{theorem}

This theory says that, if we assume exchangable data (and iid $\Rightarrow$ exchangeable), then there must exist a \(\theta\), \(p(x|\theta)\) and distribution \(P\) on \(\theta\)! So the idea of having a prior distribution on the parameters does have theory to back it up!


\subsubsection{Likelihood Principle}

The \textbf{Likelihood Principle} says that all the evidence in a sample relevent to parameters \(\theta\) is contained in the likelihood function. Furthermore, two likelihood functions contain the same information about \(\theta\) if they
are proportional to each other.\cite{slides} This principle, if you believe it, gives a good justification for Bayesianism since \(p(\theta|D) \propto P(D|\theta)P(\theta)\) (i.e. $\theta$ is being inferred from the likelihood). Frequentists do not like the Likelihood Principle as it leads to contradictions with their methodology - see the following coin tossing example \cite{MJordanNotes}.

If you have trouble accepting the Likelihood Principle, it has been shown to be equivalent to two milder principles:
\begin{enumerate}
	\item \textbf{Sufficiency Principle}: If two different observations $x$, $y$ are such that $T(x) = T(y)$ for sufficient statistic $T$, then inference based on $x$ and $y$ should be the same.
	\item \textbf{Conditionality Principle}: If an experiment concerning inference about $\theta$ is chosen from a collection of
	possible experiments independently, then any experiment not chosen is irrelevant to the inference.
\end{enumerate}

Of these, Sufficiency is accepted by both Frequentists and Bayesians, while the Conditionality principle is debated.


\section{Statistical Decision Theory}

\subsection{Formal Set-Up}

We need a general framework to make data-driven decisions under uncertainty. More formally, we observe some \textbf{Data} \(D \in \mathcal{D}\) which comes from some \textbf{Data Generating Distribution} \(D \sim P\). Let \(P\in \mathcal{P}\)\footnote{Often $P$ will describe an IID process, e.g. $D = (X_1,...,X_n)$ where $X_i \overset{iid}\sim P_0$. In this case, the loss is usually written w.r.t $P_0$ instead of $P$.}, where \(\mathcal{P}\) is the set of possible distributions. Let \(\mathcal{A}\) be our set of possible actions. To determine how good an action is, we define the \textbf{Loss} (cost) of doing that action as \(L: \mathcal{P} \times \mathcal{A} \mapsto \mathbb{R}\). The goal is to determine a \textbf{Decision Rule} \(\delta: \mathcal{D} \mapsto \mathcal{A}\) which, given data, produces an action.

Typically we consider \(\mathcal{P}\) as a Parametric Family of distributions, and we use \(\Theta\) interchangibly with \(\mathcal{P}\), using that \(P := P_{\theta}\).

\subsection{Procedure Analysis}

We need a way to assign a value to any \(\delta\) and a way to compare these values to find which one is ``best''. One such way of doing this is via the Frequentist Risk.

\subsubsection{Frequentist Risk Perspective} 
The first approach seeks to minimize the \textbf{Frequentist Risk}, which is defined as:
\begin{equation}\label{eq:1}
	R(P,\delta)=\mathbb{E}_{D\sim P}\{L(P,\delta(D))\}
\end{equation}

If we want to compare decision rules \(\delta_1, \delta_2\) using def \ref{eq:1}, we have to take into account \(P\), since \(R\) varies with both \(P\) and \(\delta\). Sometimes one decision rule \(\delta_1\) is better then another \(\delta_2\) regardless of \(P\), in which case we say \(\delta_1\) \textbf{Dominates} \(\delta_2\).
More formally:
$$R(P,\delta_1)\leq R(P,\delta_2) \forall P \in \mathcal{P}\ and$$
$$\exists P \in \mathcal{P},\ R(P,\delta_1) < R(P, \delta_2)$$

This basically means that \(\delta_1\) is a better decision rule then \(\delta_2\). Sometimes, there may be a ``best'' \(\delta\), one which isn't dominated by any other \(\delta_0\). We say \(\delta\) is \textbf{Admissible} if $\nexists \delta_0$ s.t. $\delta_0$ dominates $\delta$. Note: we should rule out inadmissible decision rules (except for simplicity or efficiency) but not necessarily accept Admissible ones!

Unfortunately, different \(P\)'s usually produce different optimal \(\delta\)'s! we must take into account the unknown \(P\) when minimizing (\ref{eq:1}). One way to take this into account is to use the \textbf{Minimax Criteria}: the optimal \(\delta\) minimizes the Frequentist Risk in worst case scenerio.

\begin{equation}
\delta_{minimax} = \,\min\limits_{\delta}\,\max\limits_{P \in \mathcal{P}}\ R(P,\delta)
\end{equation}

If \(\mathcal{P}\) is a Parameteric Family, we can handle the dependence of \(R\) on \(P\) by averaging it out, adding weights \(\pi\) over $\Theta$ to put more weight on certain $\theta$'s. We can then minimize over \(\delta\). This is called the \textbf{Bayes Risk}, even though it is Frequentist concept since it averages over $D$ via (\ref{eq:1}).
\begin{equation}\label{eq:3}
\delta_{bayes}= \text{arg}\,\min\limits_{\delta}\,\int_{\Theta}^{}R(P_\theta,\delta)\pi(\theta)d\theta
\end{equation}

Where $\delta_{bayes}$ is called the \textbf{Bayes Rule}. Note that the Bayes Rule may not exist, and when they do they may not be unique.

\subsubsection{Bayesian Risk Perspective} 

Note that (\ref{eq:1}) does not consider that we only observed one \(D\). We can define a Risk function that does. The \textbf{Posterior Risk} is
\begin{equation}\label{eq:2}R_B(\delta|D) = \int_{\Theta}^{}L(P_{\theta},\delta)p(\theta|D)d\theta
\end{equation}
Where $p(\theta|D)$ is the posterior for a given prior $\pi(\theta)$. We can choose our decision rule based on this new risk function. This is called the \textbf{Bayes Estimator} or \textbf{Bayes Action} (not to be confused with the \textbf{Bayes Rule} above).

\begin{equation}\label{eq:5}
\delta_{post}= \text{arg}\,\min\limits_{\delta}\,R_B(\delta|D)
\end{equation}


Notice that in (\ref{eq:2}), we do not consider different unobserved values of \(D\), since the Bayesian would say they are irrelevent courtesy of the Conditionality Principle. For them, only the observed $D$ matters for inference. 
Additionally, \(\theta\) is integrated out in (\ref{eq:2}), meaning that (\ref{eq:5}) gives the undisputed optimal \(\delta\)! 

Note that the Frequentist can still use (\ref{eq:5}) by interpreting it as (\ref{eq:3}) with \(\pi\) as the ``true'' prior for \(\Theta\).
We would then get that:
\begin{align*}
 \int_{\Theta}^{}R(P_\theta,\delta)\pi(\theta)d\theta &= \int_{\Theta}^{}\int_{D}L(P_\theta,\delta)P(D|\theta)P(\theta)dDd\theta \\&\overset{(a)}{=} \int_{D}^{}\int_{\Theta}L(P_\theta,\delta)P(\theta|D)P(D)d\theta dD\\
&=\int_{D}^{}R_B(\delta|D)P(D)dD
\end{align*}

Where (a) is due to Fubini's theorem (provided the integral is finite).
It turns out that a \emph{Bayes rule} can be obtained by taking the \emph{Bayes action} for each particular $D$! See \cite{PHoffNotes2} for more details.


\subsection{Types of Procedures}

\subsubsection{Parameter Estimation - Frequentist}
Given a Parametric Family $\{P_{\theta}\}_{\theta\in\Theta}$, typically we have data \(D=(X^{(1)}, \cdots, X^{(n)})\) where each \(X^{(i)}\overset{iid}{\sim} P_{\theta}\). We want to use this data to estimate the true parameters \(\theta\). Hence, \(\mathcal{A} = \Theta\) and $\delta(D)$ is some an \textbf{Estimator} of \(\theta\). The estimator should minimize some cost, for example: \(L(\theta,\delta(D)) = ||\theta-\delta(D)||^2\). Note that since the data are IID we use the marginal density over $X$ instead of the joint over $D$ in the loss function.

If we take the expectation of the loss function above (the risk), we can decompose it nicely into two pieces:

\begin{align*}
R(P,\delta)&=\mathbb{E}_{D\sim P}\{||\theta-\delta(D)||^2\}\\
&=\mathbb{E}_{D\sim P}\{(\theta-\mathbb{E}_{D\sim P}\{\delta(D)\} + \mathbb{E}_{D\sim P}\{\delta(D)\} - \delta(D))^2\}\\
&=\underbrace{(\theta-\mathbb{E}_{D\sim P}\{\delta(D)\})^2}_{Bias^2} + \underbrace{\mathbb{E}_{D\sim P}\{(\mathbb{E}_{D\sim P}\{\delta(D)\} - \delta(D))^2\}}_{Variance}
\end{align*}

The above says that when we average this loss over all possible datasets, we can compare how much of the loss is due to the Bias and how much to the Variance. This is a Frequentist idea since it involves taking an expectation over the data generating distribution, an idea doesn't appeal to Bayesians since it is contrary to the conditionality principle. 

\subsubsection{Parameter Estimation - Bayesian}

Next we show that \(\delta_{post}(D) = \mathbb{E}\{\theta|D\}\). This is a simple optimization problem:
\begin{align*}
R_B(\delta|D) &= \int_{\Theta}^{}||\theta-\delta(D)||^2p(\theta|D)d\theta \\
&=\delta(D)^2 -2\delta(D)\int_{\Theta}^{}\theta p(\theta|D)d\theta+\int_{\Theta}^{}\theta^2 p(\theta|D)d\theta
\end{align*}

and taking the derivative and setting to 0 yields:
\begin{align*}
&\frac{\partial R_B}{\partial \delta} = 2\delta(D) -2\int_{\Theta}^{}\theta p(\theta|D)d\theta = 0 \\
&\Rightarrow \delta(D) = \int_{\Theta}^{}\theta p(\theta|D)d\theta = \mathbb{E}\{\theta|D\}
\end{align*}


The loss \(L(\theta,a) = ||\theta-a||^2\) is so popular that it has its own name -- the \textbf{Squared Loss}. It is a member of a family of loss functions called the \textbf{Minkowski Loss}. This family has the following form:
\begin{equation}
	L(\theta,\delta(D))_q = ||\theta-\delta(D)||^q
\end{equation}

We have seen the decision rule for for $q=2$. We assume that $\Theta=\mathbb{R}$ and show that for $q=1$ the solution is the posterior median:

\begin{align*}
R_B(\delta|D) &= \int_{\Theta}^{}||\theta-\delta(D)||p(\theta|D)d\theta \\
&= \int_{-\infty}^{\delta(D)}(\theta-\delta(D))p(\theta|D)d\theta + \int_{\delta(D)}^{\infty}(\delta(D)-\theta)p(\theta|D)d\theta\\
&\overset{(a)}{=} \delta(D)\int_{-\infty}^{\delta(D)}p(\theta|D)d\theta - \int_{-\infty}^{\delta(D)}\theta p(\theta|D)d\theta -\\
&\hspace{10px} \lim\limits_{c \rightarrow \infty} \int_{c}^{\delta(D)}\theta p(\theta|D)d\theta + \lim\limits_{c \rightarrow \infty} \delta(D)\int_{c}^{\delta(D)}p(\theta|D)d\theta
\end{align*}

and taking the derivative and setting to 0 yields:
\begin{align*}
\frac{\partial R_B}{\partial \delta} 
&\overset{(b)}{=} \delta(D)p(\delta(D)|D)d\theta + \int_{-\infty}^{\delta(D)}p(\theta|D)d\theta - \delta(D)p(\delta(D)|D)d\theta -\\
&\hspace{10px} \delta(D)p(\delta(D)|D)d\theta + \delta(D)p(\delta(D)|D)d\theta + \lim\limits_{c \rightarrow \infty} \delta(D)\int_{c}^{\delta(D)}p(\theta|D)d\theta \\
&= \int_{-\infty}^{\delta(D)}p(\theta|D)d\theta - \lim\limits_{c \rightarrow \infty} \delta(D)\int_{\delta(D)}^{c}p(\theta|D)d\theta
\end{align*}

And equating to zero gives us

\begin{align*}
\int_{-\infty}^{\delta(D)}p(\theta|D)d\theta = \int_{\delta(D)}^{\infty}p(\theta|D)d\theta
\end{align*}

And so $\delta$ is the posterior median, as needed. For $q \rightarrow 0$ the solution is the posterior mode. 

\begin{enumerate}
	\item \(L(P_{\theta}, a) = \lim\limits_{p \rightarrow 0}\, || \theta - \delta(D) ||^p \)
\end{enumerate}


\subsubsection{Prediction}
Let $D=((X^{(1)},Y^{(1)}), \cdots, (X^{(n)},Y^{(n)}))$ where $X^{(i)} \sim \mathcal{X}$ and $Y^{(i)} \sim \mathcal{Y}$. 

We put a density on $X$ and $Y$: $(X^{(i)},Y^{(i)}) \overset{iid}{\sim} P_{XY}$. Our action space $\mathcal{A} = \mathcal{Y}^\mathcal{X}$, the set of functions $f: \mathcal{X}\mapsto \mathcal{Y}$. Hence \(\delta(D)\) is a \textbf{Learning Algorithm} which learns a function i.e. $\delta(D) = \hat{f}$. 

We can evaluate the performance of $f$ using a \emph{prediction loss} $l: \mathcal{Y} \times \mathcal{Y} \mapsto \mathbb{R}$, a measure of the distance between a given prediction and itâ€™s associated ground truth. We define the \textbf{Generalization Error} from $l$ as:
\begin{equation}\label{eq:8}
L(P,f) = \mathbb{E}_{(X,Y)\sim P_{XY}}\{l(Y,f(X)) \}
\end{equation}
This is often called the Risk in Machine Learning. Note that we do not know (\ref{eq:8}), but we can approximate it using the below formula. This is called \textbf{Empirical Risk Minimization}
\begin{equation}
L(P,f) = \frac{1}{n}\sum_{i=1}^n l\left(Y^{(i)},f(X^{(i)})\right)
\end{equation}


\newpage



\section{Information Theory}

We want a function $I$ which measures how much information you learn from observing some event $E$. We want it to satisfy some properties, mainly:

\begin{enumerate}
	\item Highly probable $E$ have low $I(E)$ and conversely $\rightarrow$ \emph{ rare events give more information}.
	\item $I(E) \ge 0$ $\rightarrow$\emph{ Information is non-negative}.
	\item if $P(E)=1$ then $I(E) = 0$ $\rightarrow$\emph{ Events that always occur provide no information}.
	\item If $E_1, E_2$ are independent events then $I(E_1 \cap E_2) = I(E_1) + I(E_2)$ $\rightarrow$\emph{ information due to independent events are additive}.
\end{enumerate}
From 1. and 3. we see that $I$ should be a function of the probability of an events occurence, i.e. $I(E)=f(P(E))$ for some $f$. From 4., given independent events $E_1, E_2$, we have that:
\begin{align}
f(P(E_1)P(E_2)) &= f(P(E_1\cap E_2)) = f(P(E_1)) + f(P(E_2))\\
f(x\cdot y) &= f(x) + f(y)
\end{align}

If we assume that $I$ is continuous, then only $I(E) = Klog P(E)$ satisfies (11)  \cite{EntNotes}. Finally, using 2., we see that $K<0$. We can then define $I$ as:

\begin{equation}
I(E) = -log P(E)
\end{equation}

Where the choice of $K$ decides the base of the logarithm.

\subsection{Entropy}
We can extend this notion to a discrete Random Variable $X\sim p$ with finite domain $\mathcal{X}$. By defining the \textbf{Shannon Entropy} $H(X)$ as the average amount of information i.e. 
\begin{equation}
H(X) = \mathbb{E}_{X\sim P}\{I(X)\} = \mathbb{E}_{X\sim P}\{-\log P(X)\} = -\sum_{x\in\mathcal{X}}p(x)\log p(x)
\end{equation}
We can also denote this as $H(p)$ where $p \sim X$ depending on what we want to emphasize. Note that WLOG we can assume that \(p(x)>0 \ \forall x\in\mathcal{X}\). This is because we can use the convention that \(0\cdot log0 = 0\) (based on continuity arguments). Hence zero probability outcomes do not contribute to $H(X)$ anyways. We can further extend this for two Random Variables $X$, $Y$ with finite domain \(\mathcal{X}\times\mathcal{Y}\) by defining the \textbf{Joint Entropy} as:
\begin{equation}
H(X,Y) = -\sum_{x,y}p_{XY}(x,y)\log p_{XY}(x,y)
\end{equation}

The \textbf{Conditional Entropy} is defined as:
\begin{equation}
H(X|Y) = \mathbb{E}_{X|Y}\{-\log p(X|Y)\} = -\sum_{x,y}p_{XY}(x,y)\log p_{X|Y}(x|y)
\end{equation}

These quantities have nice properties:
\begin{enumerate}
	\item \emph{Non-negativity}: \(H(X)\ge0\), with equality only when X is a constant.
	
	PROOF: WLOG we assume that \(p(x)>0 \ \forall x\in\mathcal{X}\). We have that \(H(X) = -\sum_{x} p(x)logp(x) = \sum_{x} p(x)logp(x)^{-1}\ge0\), since \(p(x)>0\) and \(p(x)^{-1} \ge 1\). If \(H(X)=0\) then \(\exists \alpha\) such that \(p(\alpha)^{-1}=1 \Rightarrow p(\alpha)=1\). Hence \(X\) must be a constant, as needed.
	
	\item \emph{Chain Rule}: $H(X,Y) = H(X \mid Y) + H(Y) = H(Y \mid X) + H(X)$ 
	\item \emph{Monotonicity}: $H(X\mid Y) \le H(X)$ 
\end{enumerate}

\subsection{KL Divergence}
We can now look at the \textbf{KL Divergence} or \textbf{Relative Entropy}. This quantity measures the ``distance'' between two probability mass functions $p$ and $q$.

\begin{equation}
KL(p||q) = \mathbb{E}_{X\sim p}\left\{\log \frac{p(X)}{q(X)}\right\} = \sum_{x\in\mathcal{X}}p(x)\log\frac{p(x)}{q(x)}
\end{equation}

The KL divergence has some nice properties.
\begin{enumerate}
	\item $KL(p||q) \ge 0$ with equality iff $p=q$
	
	PROOF: If there exists $x \in \mathcal{X}$ such that $p(x) = 0$ and $q(x) > 0$, then $KL(p || q) = \infty$.	Otherwise:
	\begin{align*}
	-KL(p||q) &= \mathbb{E}_{X\sim p}\left\{\log \frac{q(X)}{p(X)}\right\}\\
	&\overset{(a)}{\le} \log \mathbb{E}_{X\sim p}\left\{\frac{q(X)}{p(X)}\right\} \\
	&= \log \sum_{x} p(x)\frac{q(x)}{p(x)} = \log \sum_{x} q(x) = 0
	\end{align*}
	Where (a) follows from Jensen's inequality. $KL(p||q) = 0$ only occurs when there is equality in Jensen's inequality, which only occurs when $p(x)=cq(x)$ for some $c$. Since $\sum_{x}cq(x) = c\sum_{x}q(x) = c \Rightarrow c=1$, so $p=q$ as needed.
	
	\item $KL(p||q)$ is strictly convex in each argument
	
	\item $KL(p||q) \ne KL(q||p)$ so it is not a metric
	
	\item We can decompose the KL divergence into two seperate terms:
	
	\begin{align} KL(p||q) &= \sum_{x\in\mathcal{X}}p(x)\log p(x) - \sum_{x\in\mathcal{X}}p(x)\log q(x) \\
	&= -H(p) + \mathbb{E}_{X\sim p}\{-\log q(x)\} \\
	&= -H(p) + CE(p,q)
	\end{align}
	
	Where the $H(p)$ is the Entropy and $CE(p,q)$ is called the \textbf{Cross Entropy}.
	
\end{enumerate}

The KL divergence can be used within the Decision Theoretic framework. We elaborate in the following subsections.

\subsubsection{Cross Entropy as a Loss Function}

Semantically, $KL(p||q)$ represents how well some distribution $q$ approximates the ``true'' $p$. Suppose we wanted to estimate a distribution $p$ which we knew belonged to a Parametric Family  $p\in \{p_{\theta}\}_{\theta\in\Theta}$. We can put this into our decision theoretic framework. Let \(\mathcal{A} = \{p_{\theta}\}_{\theta\in\Theta}\) and $\delta(D)=p_{\theta}$. We can use the Cross Entropy as a loss function.

\begin{equation}\label{eq:celf}
L(p,p_{\theta}) = \mathbb{E}_{X\sim p}\{ -\log p_{\theta}(X) \}
\end{equation}	

This is because $KL(p,p_{\theta}) = -H(p) + L(p,p_{\theta})$, and since $H(p)$ is constant, minimizing the KL is equivalent to minimizing the cross entropy. Since $KL(p,p_{\theta}) \ge 0$ we see that the minimum is attained at $L(p,p_{\theta}) = H(p)$, which occurs when $p_{\theta}=p$ i.e. when our prediction matches the ``true'' density.


\subsubsection{Maximum Likelihood Estimation}

We don't know $p$, so we cannot compute (\ref{eq:celf}). Instead, we can use in its place the empirical distribution $\hat{p}$, i.e. 
\begin{equation}
\hat{p}(x) = \frac{1}{n}\sum_{i=1}^n\delta(x,x^{(i)})
\end{equation}

Given $X$ discrete, it turns out that the MLE for $\theta$ is the same as $\text{arg}\,\min\limits_{\theta\in\Theta} KL(\hat{p}||p_{\theta})$. This is because:

\begin{align*}
KL(\hat{p}||p_{\theta}) &= -H(\hat{p}) + CE(\hat{p},p_{\theta}) \\
&= -H(\hat{p}) - \sum_{x\in\mathcal{X}}\hat{p}(x)\log p_{\theta}(x)\\
&= -H(\hat{p}) - \frac{1}{n}\sum_{x\in\mathcal{X}}\sum_{i=1}^n\delta(x,x^{(i)}) \log p_{\theta}(x)\\
&= -H(\hat{p}) - \frac{1}{n}\sum_{i=1}^n\log p_{\theta}(x^{(i)})\\
&= -H(\hat{p}) - \frac{1}{n}l(\theta \mid x^{(1)}, \cdots, x^{(n)})
\end{align*}

This provides a nice interpretation for the MLE - it is finding the $p\in \{p_{\theta}\}_{\theta\in\Theta}$ which minimizes the dissimilarity between the empirical distribution of the training set and itself as measured by the KL divergence. Conversly we can justify the use of the Cross Entropy loss through its equivalence to Maximum Likelihood. Because of this, the Cross Entropy loss is also known as the \textbf{Negative Log Loss}.

On a final note, one may think that the quantity $KL(p_{\theta}|| \hat{p})$ could be interesting. They would be wrong. This is since $p_{\theta}(x)=0 \Rightarrow \hat{p}_{\theta}(x)=0$ but $\hat{p}_{\theta}(x)=0 \not\Rightarrow p_{\theta}(x)=0$ since $\hat{p}_{\theta}(x)=0$ only means that the particular value of $x$ wasn't observed in the sample.	

\subsection{Differential Entropy}
We can define the Entropy and the KL divergence for continuous random variables.

\begin{align}
H(p) &= -\int_{x\in\mathcal{X}}p(x)\log p(x)d\mu(x) \\
KL(p,q) &= \mathbb{E}_{X\sim p}\left\{\log \frac{p(X)}{q(X)}\right\} = \int_{x\in\mathcal{X}}p(x)\log\frac{p(x)}{q(x)}d\mu(x)
\end{align}

In the continuous case, the properties previously described hold except that the entropy is no longer necessarily non negative.

\subsection{Mutual Information}
We can quantify the amount of information obtained about one discrete random variable $X$, through another $Y$ by defining the \textbf{Mutual Information} as:
\begin{equation}
I(X,Y)=\sum_{x,y}p_{X,Y}(x,y)\log\frac{p_{XY}(x,y)}{p_X(x)p_Y(y)}
\end{equation}

We again assume WLOG that \(p(x,y)>0 \ \forall (x,y)\in\mathcal{X}\times\mathcal{Y}\). We note the following properties of $I$:

\begin{enumerate}
	\item \(I(X,X) = H(X)\) \(\rightarrow\) Sometimes the Entropy is called the \textbf{Self Information}
	\item $I(X,Y) = KL(p_{XY}||p_Xp_Y)$
	\item \(I(X,Y)\ge 0\)
	
	PROOF: Notice that \(I(X,Y) = KL(p_{XY}||p_Xp_Y) \ge 0\) by the positiveness of \(KL(\cdot||\cdot)\).
	
	\item $I(X,Y) = H(p_X) + H(p_Y) -H(p_{XY})$
	
	PROOF:
	\begin{align*}
	I(X,Y) &= KL(p_{XY}||p_Xp_Y) \\
	&= -H(p_{XY}) + CE(p_{XY},p_Xp_Y) \\
	&= -H(p_{XY}) - \sum_{x,y}p_{X,Y}(x,y)\log p_X(x)p_Y(y)\\
	&= -H(p_{XY}) - \left(\sum_{x,y}p_{X,Y}(x,y)\log p_X(x) + \sum_{x,y}p_{X,Y}(x,y)\log p_Y(y)\right)\\
	&=-H(p_{XY}) - \left(\sum_{x}p_{X}(x)\log p_X(x) + \sum_{y}p_{Y}(y)\log p_Y(y)\right)\\
	&= -H(p_{XY}) + H(p_X) + H(p_Y)
	\end{align*}
\end{enumerate}




\subsection{Maximum Entropy Principle}


\subsubsection{Examples}
We can show that for discrete spaces, the uniform density is the Maximum Entropy.
Let \(X \sim p\) and $Dom(X) = \mathcal{X}$ with $k$ elements and \(q\sim Unif\) on $\mathcal{X}$. Then \(D(p||q) = -H(X) + logk\)

PROOF: 
\begin{align*}
D(p||q)&=\sum_{x}p(x)log\frac{p(x)}{q(x)}\\
&=\sum_{x}p(x)logp(x)-\sum_xp(x)logq(x)\\
&= -H(X) + \sum_xp(x)logk \\
&= -H(x) + logk
\end{align*}
An upper bound for \(H(X)\) is \(logk\) since \(H(X)=logk-D(p||q) \Rightarrow H(X) \le logk \).

The maximum entropy solution for Random Variables is Independence
From the previous result we have that \(I(X_1,X_2)\ge0 \Rightarrow H(X_1) + H(X_2) \ge H(X_1,X_2) \), and so the maximal entropy of \((X_1,X_2)\) is \(H(X_1) + H(X_2)\). By definition this only occurs when \(I(X_1,X_2)=0\), which only occurs if
\(p_{1,2}(x_1,x_2)=p_1(x_1)p_2(x_2) \ \forall x_1,x_2 \in \mathcal{X}_1\times\mathcal{X}_2\). This can be seen directly from the definition of \(I\) and using the strict positivity of \(p(x_1,x_2)\).

The maximum entropy solution for continuous random variables with given mean and variance is Normal.

\newpage

\bibliography{Note1.bib}
\bibliographystyle{ieeetr}

\end{document}