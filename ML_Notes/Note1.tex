\documentclass[]{article}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathreplacing}
\usepackage[hidelinks]{hyperref}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{xcolor} % For color
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
%\usepackage{enumerate} % For lettered enumeration

\begin{document}

\title{Machine Learning Notes}
\author{Matthew Scicluna}
\maketitle

\section{Why Bayesian?}

Given an Event, what does the probability of that Event \emph{mean}? There are two interpretations:

\begin{enumerate}
	\item \textbf{Frequentists}: the \emph{limiting frequency} of the event
	\item \textbf{Bayesians}: the \emph{reasonable expectation} that the event occurs
\end{enumerate}

The \emph{reasonable expectation} can be further broken down into two views. The \textbf{Objective Bayesians} view the \emph{reasonable expectation} as the \emph{state of knowledge}. They view probability as an extension of propositional logic, which is described in \cite{jaynes}. The \textbf{Subjective Bayesians} view probability as a quantification of \emph{personal belief}. The main difference between the groups is in how they choose their priors: the Subjective Bayesians use knowledge about or prior experience with model parameters, wheras the Objectivists try to introduce as little prior knowledge as possible, using noninformative priors.

\subsection{Existence Of The Prior}
We need a theoretical justification for why we assume the existence of a prior distribution on \(\theta\) when doing Bayesian statistics. The theoretical justification requires the following assumption about the data. We say a sequence of random variables \(x_1, \cdots\) are \textbf{Infinitely Exchangable} if for any \(n\), and any permutation of size \(n\) \(\pi_{1:n}\)
\begin{equation}
p(x_1, \cdots, x_n) = p(x_{\pi_1}, \cdots, x_{\pi_n})
\end{equation}

The \textbf{De Finetti Theorem} says that a sequence is infinitely exchangible iff for any \(n\), 
\begin{equation}
p(x_1, \cdots x_n) = \int\prod_{i=1}^n p(x_i|\theta)P(d\theta)
\end{equation}
For some measure \(P\) on \(\theta\). 
\\
Note: if $\theta$ has a density then $P(d\theta) = p(\theta)d\theta$. What this theory means is that given the assumption of exchangable data (and iid $\Rightarrow$ exchangable) then there must exist a \(\theta\), \(p(x|\theta)\) and distribution \(P\) on \(\theta\). Also note: \(\theta\) may be infinite!

\subsection{Likelihood Principle}

The \textbf{Likelihood Principle} says that all the evidence in a sample relevent to parameters \(\theta\) is contained in the likelihood function. Furthermore, two likelihood functions contain the same information about \(\theta\) if they
are proportional to each other.\cite{slides} This principle, if you believe it, gives a good justification for Bayesianism since \(p(\theta|D) \propto P(D|\theta)P(\theta)\) (i.e. $\theta$ is being inferred from the likelihood). Frequentists do not like the Likelihood Principle as it leads to contradictions with their methodology - see the following coin tossing example \cite{MJordanNotes}.

If you have trouble accepting the Likelihood Principle, it has been shown to be equivalent to two other principles:
\begin{enumerate}
	\item \textbf{Sufficiency Principle}: If two different observations $x$, $y$ are such that $T(x) = T(y)$ for sufficient statistic $T$, then inference based on $x$ and $y$ should be the same.
	\item \textbf{Conditionality Principle}: If an experiment concerning inference about $\theta$ is chosen from a collection of
	possible experiments independently, then any experiment not chosen is irrelevant to the inference.
\end{enumerate}

Of these, Sufficiency is accepted by both Frequentists and Bayesians, while the Conditionality principle is debated.


\section{Statistical Decision Theory}

\subsection{Formal Set-Up}

We need a general framework to make data-driven decisions under uncertainty. More formally, we observe some \textbf{Data} \(D \in \mathcal{D}\) which comes from some \textbf{Data Generating Distribution} \(D \sim P\). Let \(P\in \mathcal{P}\)\hyperref[sec:ft1]{$^1$}, where \(\mathcal{P}\) is the set of possible distributions. Let \(\mathcal{A}\) be our set of possible actions. To determine how good an action is, we define the \textbf{Loss} (cost) of doing that action as \(L: \mathcal{P} \times \mathcal{A} \mapsto \mathbb{R}\). The goal is to determine a \textbf{Decision Rule} \(\delta: \mathcal{D} \mapsto \mathcal{A}\) which, given data, produces an action.

Typically we consider \(\mathcal{P}\) as a Parametric Family of distributions, and we use \(\Theta\) interchangibly with \(\mathcal{P}\), using that \(P := P_{\theta}\).

\subsection{Procedure Analysis}

We need a way to assign a value to any \(\delta\) and a way to compare these values to find which one is ``best''. One such way of doing this is via the Frequentist Risk.

\subsubsection{Frequentist Risk Perspective} 
The first approach seeks to minimize the \textbf{Frequentist Risk}  
\begin{equation}\label{eq:1}
	R(P,\delta)=\mathbb{E}_{D\sim P}\{L(P,\delta(D))\}
\end{equation}
If we want to compare decision rules \(\delta_1, \delta_2\) using (\ref{eq:1}), we have to take into account \(P\), since \(R\) varies with both \(P\) and \(\delta\). Sometimes one decision rule \(\delta_1\) is better then another \(\delta_2\) regardless of \(P\), in which case we say \(\delta_1\) \textbf{Dominates} \(\delta_2\).
More formally:
$$R(P,\delta_1)\leq R(P,\delta_2) \forall P \in \mathcal{P}\ and$$
$$\exists P \in \mathcal{P},\ R(P,\delta_1) < R(P, \delta_2)$$

This basically means that \(\delta_1\) is a better decision rule then \(\delta_2\). Sometimes, there may be a ``best'' \(\delta\), one which isn't dominated by any other \(\delta_0\). We say \(\delta\) is \textbf{Admissible} if $\nexists \delta_0$ s.t. $\delta_0$ dominates $\delta$. Note: we should rule out inadmissible decision rules (except for simplicity or efficiency) but not necessarily accept Admissible ones!

Unfortunately, different \(P\)'s usually produce different optimal \(\delta\)'s! we must take into account the unknown \(P\) when minimizing (\ref{eq:1}). One way to take this into account is to use the \textbf{Minimax Criteria}: the optimal \(\delta\) minimizes the Frequentist Risk in worst case scenerio

\begin{equation}
\delta_{minimax} = \,\min\limits_{\delta}\,\max\limits_{P \in \mathcal{P}}\ R(P,\delta)
\end{equation}

If \(\mathcal{P}\) is a Parameteric Family, we can handle the dependence of \(R\) on \(P\) by averaging it out, adding weights \(\pi\) over $\Theta$ to put more weight on certain $\theta$'s. We can then minimize over \(\delta\). This is called the \textbf{Bayes Risk}, even though it is Frequentist concept since it averages over $D$ via (\ref{eq:1}).
\begin{equation}\label{eq:3}
\delta_{bayes}= \text{arg}\,\min\limits_{\delta}\,\int_{\Theta}^{}R(P_\theta,\delta)\pi(\theta)d\theta
\end{equation}

Where $\delta_{bayes}$ is called the \textbf{Bayes Rule}. Note that the Bayes Rule may not exist, and when they do they may not be unique.

\subsubsection{Bayesian Risk Perspective} 

Note that (\ref{eq:1}) does not consider that we only observed one \(D\). We can define a Risk function that does. The \textbf{Posterior Risk} is
\begin{equation}\label{eq:2}R_B(\delta|D) = \int_{\Theta}^{}L(P_{\theta},\delta)p(\theta|D)d\theta
\end{equation}
Where $p(\theta|D)$ is the posterior for a given prior $\pi(\theta)$. We can choose our decision rule based on this new risk function. This is called the \textbf{Bayes Estimator} or \textbf{Bayes Action} (not to be confused with the \textbf{Bayes Rule} above).

\begin{equation}\label{eq:5}
\delta_{post}= \text{arg}\,\min\limits_{\delta}\,R_B(\delta|D)
\end{equation}


Notice that in (\ref{eq:2}), we do not consider different unobserved values of \(D\), since the Bayesian would say they are irrelevent courtesy of the Conditionality Principle. For them, only the observed $D$ matters for inference. 
Additionally, \(\theta\) is integrated out in (\ref{eq:2}), meaning that (\ref{eq:5}) gives the undisputed optimal \(\delta\)! 

Note that the Frequentist can still use (\ref{eq:5}) by interpreting it as (\ref{eq:3}) with \(\pi\) as the ``true'' prior for \(\Theta\).
We would then get that:
\begin{align*}
 \int_{\Theta}^{}R(P_\theta,\delta)\pi(\theta)d\theta &= \int_{\Theta}^{}\int_{D}L(P_\theta,\delta)P(D|\theta)P(\theta)dDd\theta \\&\overset{(a)}{=} \int_{D}^{}\int_{\Theta}L(P_\theta,\delta)P(\theta|D)P(D)d\theta dD\\
&=\int_{D}^{}R_B(\delta|D)P(D)dD
\end{align*}

Where (a) is due to Fubini's theorem (provided the integral is finite).
It turns out that a \emph{Bayes rule} can be obtained by taking the \emph{Bayes action} for each particular $D$! See \cite{PHoffNotes2} for more details.

\subsubsection{Examples}
An example of a Bayes Action is: given \(\mathcal{A} = \Theta\) and \(L(\theta,a) = ||\theta-a||^2\) we have that \(\delta_{post}(D) = \mathbb{E}\{\theta|D\}\). This is because:
\begin{align*}
R_B(\delta|D) &= \int_{\Theta}^{}||\theta-\delta(D)||^2p(\theta|D)d\theta \\
&=\delta(D)^2 -2\delta(D)\int_{\Theta}^{}\theta p(\theta|D)d\theta+\int_{\Theta}^{}\theta^2 p(\theta|D)d\theta
\end{align*}

and taking the derivative and setting to 0 yields:
\begin{align*}
&\frac{\partial R_B}{\partial \delta} = 2\delta(D) -2\int_{\Theta}^{}\theta p(\theta|D)d\theta = 0 \\
&\Rightarrow \delta(D) = \int_{\Theta}^{}\theta p(\theta|D)d\theta = \mathbb{E}\{\theta|D\}
\end{align*}

\newpage



\section{Information Theory}

We want a function $I$ which measures how much information you learn from observing some event $E$. We want it to satisfy some properties, mainly:

\begin{enumerate}
	\item Highly probable $E$ have low $I(E)$ and conversely $\rightarrow$ \emph{ rare events give more information}.
	\item $I(E) \ge 0$ $\rightarrow$\emph{ Information is non-negative}.
	\item if $P(E)=1$ then $I(E) = 0$ $\rightarrow$\emph{ Events that always occur provide no information}.
	\item If $E_1, E_2$ are independent events then $I(E_1 \cap E_2) = I(E_1) + I(E_2)$ $\rightarrow$\emph{ information due to independent events are additive}.
\end{enumerate}

It turns out that if we assume that $I$ is continuous, only one function satisfies the above \cite{EntNotes}
\begin{equation}
I(E) = -log P(E)
\end{equation}


\subsection{Entropy}
We can extend this notion to a discrete Random Variable $X\sim P$ with finite domain $\mathcal{X}$. By defining the \textbf{Shannon Entropy} $H(X)$ as the average amount of information i.e. 
\begin{equation}
H(X) = \mathbb{E}_{X\sim P}\{I(X)\} = \mathbb{E}_{X\sim P}\{-\log P(X)\} = -\sum_{x\in\mathcal{X}}p(x)\log p(x)
\end{equation}
We can also denote this as $H(p)$ where $p \sim X$ depending on what we want to emphasize. Note that WLOG we can assume that \(p(x)>0 \ \forall x\in\mathcal{X}\). This is because we can use the convention that \(0\cdot log0 = 0\) (based on continuity arguments). Hence zero probability outcomes do not contribute to $H(X)$ anyways. We can further extend this for two Random Variables $X$, $Y$ with finite domain \(\mathcal{X}\times\mathcal{Y}\) by defining the \textbf{Joint Entropy} as:
\begin{equation}
H(X,Y) = -\sum_{x,y}P_{XY}(x,y)\log P_{XY}(x,y)
\end{equation}

The \textbf{Conditional Entropy} is defined as:
\begin{equation}
H(X|Y) = \mathbb{E}_{X|Y}\{-\log P(X|Y)\} = -\sum_{x,y}P_{XY}(x,y)\log P_{X|Y}(x|y)
\end{equation}

These quantities have nice properties:
\begin{enumerate}
	\item \emph{Non-negativity}: \(H(X)\ge0\), with equality only when X is a constant.
	
	PROOF: WLOG we assume that \(p(x)>0 \ \forall x\in\mathcal{X}\). We have that \(H(X) = -\sum_{x} p(x)logp(x) = \sum_{x} p(x)logp(x)^{-1}\ge0\), since \(p(x)>0\) and \(p(x)^{-1} \ge 1\). If \(H(X)=0\) then \(\exists \alpha\) such that \(p(\alpha)^{-1}=1 \Rightarrow p(\alpha)=1\). Hence \(X\) must be a constant, as needed.
	
	\item \emph{Chain Rule}: $H(X,Y) = H(X \mid Y) + H(Y) = H(Y \mid X) + H(X)$ 
	\item \emph{Monotonicity}: $H(X\mid Y) \le H(X)$ 
\end{enumerate}

We can extend these Entropy definitions to the continuous case, where the above properties hold - except for non negativity.

\subsection{KL Divergence}
We can now look at the \textbf{KL Divergence} or \textbf{Relative Entropy}. This quantity measures the distance between two probability mass functions $p$ and $q$.
\begin{equation}
KL(p||q) = \mathbb{E}_{X\sim p}\left\{\log \frac{p(X)}{q(X)}\right\} = \sum_{x\in\mathcal{X}}p(x)\log\frac{p(x)}{q(x)}
\end{equation}

The KL divergence has some nice properties.
\begin{enumerate}
	\item $KL(p||q) \ge 0$
	
	PROOF: Use that
	\begin{align*}
	-KL(p||q) &= \mathbb{E}_{X\sim p}\left\{\log \frac{q(X)}{p(X)}\right\}\\
	&\le \log \mathbb{E}_{X\sim p}\left\{\frac{q(X)}{p(X)}\right\} \\
	&= \log \sum_{x} p(x)\frac{q(x)}{p(x)} = \log \sum_{x} q(x) = 0
	\end{align*}
	
	\item $KL(p||q)$ is strictly convex in each argument
\end{enumerate}

We can decompose the KL divergence into two seperate terms:

\begin{align*} KL(p||q) &= \sum_{x\in\mathcal{X}}p(x)\log p(x) - \sum_{x\in\mathcal{X}}p(x)\log q(x) \\
&= -H(p) + \mathbb{E}_{X\sim p}\{-\log q(x)\}
\end{align*}

Where the first term is the Entropy and the second term is called the \textbf{Cross Entropy}. The Cross Entropy makes a good Loss function.

\subsubsection{Maximum Likelihood Estimation}

Given a Parametric Family $\{P_{\theta}\}_{\theta\in\Theta}$ it turns out that the MLE for $\theta$ is the same as $\text{arg}\,\min\limits_{\theta\in\Theta}
KL(\hat{p},p_{\theta})$, where $\hat{p}$ is the empirical distribution, i.e. 
\begin{equation}
\hat{p}(x) = \frac{1}{n}\sum_{i=1}^n\delta(x,x^{(i)})
\end{equation}

This is since:
\begin{align*}
KL(\hat{p},p_{\theta}) &= -H(\hat{p}) + \mathbb{E}_{X\sim \hat{p}}\{-\log p_{\theta}(x)\} \\
&= -H(\hat{p}) - \sum_{x\in\mathcal{X}}\hat{p}(x)\log p_{\theta}(x)\\
&= -H(\hat{p}) - \frac{1}{n}\sum_{x\in\mathcal{X}}\sum_{i=1}^n\delta(x,x^{(i)}) \log p_{\theta}(x)\\
&= -H(\hat{p}) - \frac{1}{n}\sum_{i=1}^n\log p_{\theta}(x^{(i)})\\
&= -H(\hat{p}) - \frac{1}{n}\mathcal{L}(\theta \mid x^{(1)}, \cdots, x^{(n)})
\end{align*}

\subsection{Mutual Information}
We can quantify the amount of information obtained about one random variable $X$, through another $Y$ by defining the \textbf{Mutual Information} as:
\begin{equation}
I(X,Y)=\sum_{(x,y)\in\mathcal{X}\times\mathcal{Y}}p_{X,Y}(x,y)log\frac{p_{XY}(x,y)}{p_X(x)p_Y(y)}
\end{equation}

We again assume WLOG that \(p(x_1,x_2)>0 \ \forall (x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2\)

\begin{enumerate}
	\item CLAIM: \(I(X_1,X_2)\ge 0\)
	
	PROOF: Notice that \(I(X_1,X_2) = D(p_{1,2}||p_1p_2) \ge 0\) by the positiveness of \(D(\cdot||\cdot)\).
	
	\item We want to express  \(I(X_1,X_2)\) as a function of \(H(X_1), H(X_2)\) and \(H(X_1,X_2)\).
	\begin{align*}
	I(X_1,X_2) &= \sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2}p_{1,2}(x_1,x_2)log\frac{p_{1,2}(x_1,x_2)}{p_1(x_1)p_2(x_2)} \\
	&=\sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2}p_{1,2}(x_1,x_2)logp_{1,2}(x_1,x_2)-p_{1,2}(x_1,x_2)logp_1(x_1)p_2(x_2)\\
	&=-H(X_1,X_2) - \sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2}\bigg( p_1(x_1)p_2(x_2)logp_1(x_1) - p_1(x_1)p_2(x_2)logp_2(x_2) \bigg) \\
	&=-H(X_1,X_2) - \sum_{j=1}^{2}\sum_{(x_1,x_2)\in\mathcal{X}_1\times\mathcal{X}_2}p_1(x_1)p_2(x_2)logp_j(x_j)\\
	&= -H(X_1,X_2) -\sum_{j=1}^{2}\sum_{x_j\in\mathcal{X}_j}p_j(x_j)logp_j(x_j)\\
	&= -H(X_1,X_2) + H(X_1) + H(X_2)
	\end{align*}
	And so we can represent \(I(X_1,X_2)\) using \(H(X_1), H(X_2)\) and \(H(X_1,X_2)\), as needed.
	\item From the previous result we have that \(I(X_1,X_2)\ge0 \Rightarrow H(X_1) + H(X_2) \ge H(X_1,X_2) \), and so the maximal entropy of \((X_1,X_2)\) is \(H(X_1) + H(X_2)\). By definition this only occurs when \(I(X_1,X_2)=0\), which only occurs if
	\(p_{1,2}(x_1,x_2)=p_1(x_1)p_2(x_2) \ \forall x_1,x_2 \in \mathcal{X}_1\times\mathcal{X}_2\). This can be seen directly from the definition of \(I\) and using the strict positivity of \(p(x_1,x_2)\).
\end{enumerate}


Mutual Information \(I(X,X) = H(X)\)

\subsection{Maximum Entropy Principle}
\textbf{Density Estimation} - Given a Parametric Family \(\{P_{\theta}\}_{\theta\in\Theta}\) we would like to use data \(X\) to choose a \(P\) from our Family.

We can show that for discrete spaces, the uniform density is the Maximum Entropy.
Let \(X \sim p\) and $Dom(X) = \mathcal{X}$ with $k$ elements and \(q\sim Unif\) on $\mathcal{X}$. Then \(D(p||q) = -H(X) + logk\)

PROOF: 
\begin{align*}
D(p||q)&=\sum_{x}p(x)log\frac{p(x)}{q(x)}\\
&=\sum_{x}p(x)logp(x)-\sum_xp(x)logq(x)\\
&= -H(X) + \sum_xp(x)logk \\
&= -H(x) + logk
\end{align*}
An upper bound for \(H(X)\) is \(logk\) since \(H(X)=logk-D(p||q) \Rightarrow H(X) \le logk \).

\newpage

\section{Notes}
\begin{enumerate}
	\item \label{sec:ft1} Often $P$ will describe an IID process, e.g. $D = (X_1,...,X_n)$ where $X_i \overset{iid}\sim P_0$. In this case, the loss is usually written w.r.t $P_0$ instead of $P$.
\end{enumerate}

\bibliography{Note1.bib}
\bibliographystyle{ieeetr}

\end{document}