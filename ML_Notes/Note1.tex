\documentclass[]{article}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathreplacing}
\usepackage[hidelinks]{hyperref}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{xcolor} % For color
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
%\usepackage{enumerate} % For lettered enumeration
\usepackage{amsthm}

%  Make theorems look nice
\newtheoremstyle{mattstyle}%                % Name
{}%                                     % Space above
{}%                                     % Space below
{\itshape}%                                     % Body font
{}%                                     % Indent amount
{\bfseries}%                            % Theorem head font
{.}%                                    % Punctuation after theorem head
{ }%                                    % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%      

%  State and prove theorems
\theoremstyle{mattstyle}
\newtheorem{theorem}{Theorem}[section]

%  Definitions
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{document}

\title{Machine Learning Notes}
\author{Matthew Scicluna}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Statistics, Probability and Optimization Background}

We discuss the statistical background of Machine Learning and introduce the ideas that will be used throughout these notes.

\subsection{Useful Statistical Ideas}

\subsubsection{Empirical Distribution}\label{sec:empdist}

Given some data \(x_1, \cdots x_n \sim F\) where $F$ is an unknown CDF, we want to approximate this using some mapping $\hat{F}$ called the \textbf{Empirical Distribution} of the data.

\begin{equation}
	\hat{F}(t) = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{x_i \le t\}}
\end{equation} 

It can be shown that $\hat{F}(t) \rightarrow F(t) \ a.s.\ \forall t$, justifying its use as an approximation of $F$, provided enough data has been observed. As with $F$ we can approximate $f$. We define the \textbf{Empirical Density Function} $\hat{f}$:

\begin{equation}
	\hat{f}(t) = \frac{1}{n}\sum_{i=1}^n \delta(x_i, t)
\end{equation}

Where $\delta$ is defined differently in the continuous and discrete case. In the continuous case it is called the \textbf{Dirac Delta Function}:
	\begin{equation}
		\delta(x,y) = \begin{cases}
		\infty & x=y \\
		0 & \text{o.w.}
	\end{cases}
	\end{equation}
	Additionally, we suppose that:
	\begin{enumerate}
		\item $\int_{-\infty}^{\infty} \delta(t,y)dt = 1$
		\item $\int \delta(t,y) f(t) dt = f(y)$, for any $f$ with compact support that is continuous around $y$
	\end{enumerate}

This is not a function, but is called a \emph{Generalized Function}. In the discrete case things are much simpler, as we can use the simpler \textbf{Kronecker delta function}:
\begin{equation}
	\delta(x,y) = 
	\begin{cases}
		1 & x=y \\
		0 & \text{o.w.}
	\end{cases}
\end{equation}

Finally, we notice that $\hat{f}$ and $\hat{F}$ satisfy an important relationship that would be expected from the cdf and pdf: $\int_{-\infty}^{t}\hat{f}(y)dy = \hat{F}(t)$.

\begin{align*}
\int_{-\infty}^{t}\hat{f}(y)dy &= \int_{-\infty}^{t} \frac{1}{n}\sum_{i=1}^n \delta(x_i, y)dy\\
&= \frac{1}{n}\sum_{i=1}^n \int_{-\infty}^{t}  \delta(x_i, y)dy\\
&= \frac{1}{n}\sum_{i=1}^n \int_{-\infty}^{\infty} \mathbbm{1}_{\{x_i \le y\}} \delta(x_i, y)dy\\
&= \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{x_i \le y\}}\\
&= \hat{F}(t)
\end{align*}

\subsubsection{Exponential Family}\label{sec:expfam}

The \textbf{(Canonical) Exponential Family} is a parametric family of distributions which have the following form:
\begin{align*}
p(x|\eta) = \exp\{ \eta^TT(x) - A(\eta)\}h(x)
\end{align*}
It is defined by two quantities
\begin{itemize}
	\item $h(x)d\mu(x)$ $\rightarrow$ the \textbf{reference measure} on $X$
	\item $T: \mathcal{X} \rightarrow \mathbb{R}^p$ $\rightarrow$ the \textbf{Sufficient Statistics}
\end{itemize}

$h(x)$ is called the \textbf{Reference Density}, $d\mu(x)$ is the \textbf{Base Measure} (the Counting measure for discrete $\mathcal{X}$ and the Lebesgue measure for continuous $\mathcal{X}$). $\eta$ is called the \textbf{Canonical Parameter},  and $A(\nu)$ is the \textbf{Cumulant Function}.

\newpage
\subsection{Bayesian Statistics}

Given an Event, what does the probability of that Event \emph{mean}? There are two interpretations:

\begin{enumerate}
	\item \textbf{Frequentists}: the \emph{limiting frequency} of the event
	\item \textbf{Bayesians}: the \emph{reasonable expectation} that the event occurs
\end{enumerate}

The \emph{reasonable expectation} can be further broken down into two views. The \textbf{Objective Bayesians} view the \emph{reasonable expectation} as the \emph{state of knowledge}. They view probability as an extension of propositional logic, which is described in \cite{jaynes}. The \textbf{Subjective Bayesians} view probability as a quantification of \emph{personal belief}. The main difference between the groups is in how they choose their priors: the Subjective Bayesians use knowledge about or prior experience with model parameters, wheras the Objectivists try to introduce as little prior knowledge as possible, using noninformative priors.

\subsubsection{Existence Of The Prior}
We need a theoretical justification for why we assume the existence of a prior distribution on \(\theta\) in the first place! The justification for this requires the \textbf{Infinite Exchangeable} assumption of the data \(\{x_i\}_{i=1}^{\infty}\). This is satisfied when, given a sequence of random variables, any finite subset \(\{x_j\}_{j=1}^{n}\), and any permutation of this subset \(\pi_{1:n}\) 
\begin{equation}
p(x_1, \cdots, x_n) = p(x_{\pi_1}, \cdots, x_{\pi_n})
\end{equation}

It turns out the above is equivalent to assuming the existence of the prior! The following theorem makes this precise.

\begin{theorem}[De Finetti Theorem]
	A sequence is Infinite Exchangeable iff for any \(n\)
	$$ p(x_1, \cdots x_n) = \int\prod_{i=1}^n p(x_i|\theta)P(d\theta) $$
	for some measure \(P\) on \(\theta\). Also, if $\theta$ has a density then $P(d\theta) = p(\theta)d\theta$. Note: \(\theta\) may be infinite!
\end{theorem}

This theory says that, if we assume exchangable data (and iid $\Rightarrow$ exchangeable), then there must exist a \(\theta\), \(p(x|\theta)\) and distribution \(P\) on \(\theta\)! So the idea of having a prior distribution on the parameters does have theory to back it up!


\subsubsection{Likelihood Principle}

The \textbf{Likelihood Principle} says that all the evidence in a sample relevent to parameters \(\theta\) is contained in the likelihood function. Furthermore, two likelihood functions contain the same information about \(\theta\) if they
are proportional to each other.\cite{slides} This principle, if you believe it, gives a good justification for Bayesianism since \(p(\theta|D) \propto P(D|\theta)P(\theta)\) (i.e. $\theta$ is being inferred from the likelihood). Frequentists do not like the Likelihood Principle as it leads to contradictions with their methodology - see the following coin tossing example \cite{MJordanNotes}.

If you have trouble accepting the Likelihood Principle, it has been shown to be equivalent to two milder principles:
\begin{enumerate}
	\item \textbf{Sufficiency Principle}: If two different observations $x$, $y$ are such that $T(x) = T(y)$ for sufficient statistic $T$, then inference based on $x$ and $y$ should be the same.
	\item \textbf{Conditionality Principle}: If an experiment concerning inference about $\theta$ is chosen from a collection of
	possible experiments independently, then any experiment not chosen is irrelevant to the inference.
\end{enumerate}

Of these, Sufficiency is accepted by both Frequentists and Bayesians, while the Conditionality principle is debated.

\newpage
\subsection{Optimization}\label{sec:optim}

We now discuss how to solve optimization problems. We want to minimize our \textbf{Objective Function} $f_0 : \mathcal{D} \rightarrow \mathbb{R}$ w.r.t some  \textbf{Optimization Variable} $x \in \mathbb{R}^n$ subject to some \textbf{Inequality Constraints} $f_i: \mathbb{R}^n \rightarrow \mathbb{R}$ and some \textbf{Equality Constraints} $h_i: \mathbb{R}^n \rightarrow \mathbb{R}$.
We set $\mathcal{D} = \{x : f_0(x) < \infty\}$.
Formally:
\begin{align*}
\text{Minimize } & f_0(x) \\
\text{subject to } & f_i(x) \le 0 \ i = 1, \cdots, m\\
& h_i(x) = 0 \ i = 1, \cdots, p
\end{align*}

We call a point $x$ \textbf{Feasible} if $x\in \mathcal{D}$ and $x$ satisfies the constraints. We call $x$ \textbf{Strictly Feasible} if $x\in$ int($\mathcal{D}$) and satisfies $f_1(x)<0, \cdots, f_m(x)<0$ and $h_1(x)=0, \cdots, h_p(x)=0$. The \textbf{Optimal Value} of this problem is $p^*=\inf\{f_0(x): \text{x satisfies constraints}\}$. We let $p* = \infty$ if there are no feasible points, and $p^*=-\infty$ if the problem is unbounded from below. If $x$ feasible and $f_0(x) = p^*$ then we call it \textbf{Optimal}.

\subsubsection{Convexity}


\subsubsection{Langrangian Duality}
We can solve the above problem using the \textbf{Lagrangian} $L: \mathbb{R}^n\times \mathbb{R}^m\times \mathbb{R}^p \rightarrow \mathbb{R} $ with domain $\mathcal{D}\times\mathbb{R}^m\times \mathbb{R}^p$
\begin{equation}\label{eq:primal}
L(x,\lambda,\nu) = f_0(x) + \sum_{i=1}^m \lambda_if_i(x) + \sum_{i=1}^p \nu_i h_i(x)
\end{equation}

This problem may be difficult to solve. The problem can be simplified by introducing the \textbf{Lagrange dual function} $g: \mathbb{R}^m\times \mathbb{R}^p \rightarrow \mathbb{R}$
\begin{equation}\label{eq:dual}
g(\lambda,\nu) =\inf\limits_{x\in D} L(x,\lambda,\nu)
\end{equation}

We call (\ref{eq:primal}) the \textbf{Primal Problem} and (\ref{eq:dual}) the \textbf{Dual Problem}. We note that $g$ is concave (regardless of $f_0$) and can be $-\infty$. We call $(\lambda, \nu)$ \textbf{Dual Feasible} if $\lambda \ge 0$ and $(\lambda, \nu) \in$ dom($g$), where dom($g$)=$\{ (\lambda, \nu) | g(\lambda, \nu)>-\infty\}$ \footnote{We note that, we could add extra constraints to prevent $g(\lambda, \nu)=-\infty$ and this would not change anything. See Boyd section 5.2.1 \cite{Boyd:2004:CO:993483}}. We denote $d^*$ as the \textbf{Dual Optimum Value}: the infemum of $g$. We say that $(\lambda,\nu)$ is \textbf{Dual Optimum} if it is dual feasible and $g(\lambda,\nu)=d^*$. We now relate the primal and dual problems:
\begin{theorem}[Lower Bound Property]
	Let $\lambda \ge 0$, then $g(\lambda, \nu) \le p^*$ 
\end{theorem}
\begin{proof}
	Note that for any feasible $\tilde{x}$ and $\lambda \ge 0$:
	$$f_0(\tilde{x}) \ge L(\tilde{x}, \lambda, \nu) \ge \inf\limits_{x\in D} L(x,\lambda,\nu) = g(\lambda, \nu)$$
	and since $p^*$ is the infemum of all feasible $\tilde{x}$, it follows that $ p^* \ge g(\lambda, \nu)$
\end{proof}

Instead of minimizing $f_0$ to get $p^*$ we can maximize the lower bound $g$. This may be an easier problem since $g$ is always concave. It is clear that we always have \textbf{Weak Duality} $p^* \ge d^*$, although we want \textbf{Strong Duality}: $p^* = d^*$. A sufficient condition for strong duality is \textbf{Slater's Condition}. This requires the primal (\ref{eq:primal}) to be convex, meaning that $f_0, f_1, \cdots, f_m$ are convex and $h_i$ are \textbf{Affine}: $h(x) = 0$ can be written as $Ax = b$, where $A \in \mathbb{R}^{d \times n}$, $b\in\mathbb{R}^n$.

\begin{theorem}[Slater's Condition]
	Suppose we have a convex primal. If there exists a strictly feasible $x$ we then have strong duality
\end{theorem}
\begin{proof}
	Boyd section 5.3.2 \cite{Boyd:2004:CO:993483}
\end{proof}

\subsubsection{Saddle Point Interpretation}

Given a $w^*\in W$, $z^*\in Z$ we say that $(w^*,z^*)$ is a \textbf{Saddle Point} for function $f$ with domain $W \times Z$ if $f(w^*, z) \le f(w^*, z^*) \le f(w, z^*)$ for all $(w,z) \in W \times Z$. This means that $w^*$ minimizes $f(w, z^*)$ over $W$ and $z^*$ maximizes $f(w^*, z)$ over $Z$:
\begin{align*}
f(w^*, z^*) = \inf_{w \in W} f(w, z^*) = \sup_{z \in Z} f(w^*, z)
\end{align*}

First we notice the following:
\begin{align*}
\sup\limits_{\substack{\lambda\ge0 \\ \nu}} L(x, \lambda, \nu) &= \sup\limits_{\substack{\lambda\ge0 \\ \nu}} \left( f_0(x) + \sum_{i=1}^m \lambda_if_i(x) + \sum_{i=1}^p \nu_i h_i(x) \right)\\
&= \begin{cases}
f_0(x) & \text{$x$ is feasible}\\
\infty & \text{o.w.}
\end{cases}
\end{align*}

We can show this in cases. Suppose $x$ is feasible. $h_i(x)=0$ so we can ignore them. Since $f_i(x) \le 0,\ i=1, \cdots, m$; $\lambda = 0$ would optimize $L$ since $\sum_{i=1}^m \lambda_if_i(x) \le 0$. Now suppose $x$ was infeasible and WLOG suppose that $ \exists j \ s.t.\ f_j(x) > 0$, then we can make $L$ arbitrarily large by taking $\lambda_j \rightarrow \infty$. The same reasoning works for any $h_i(x) \ne 0$. Hence the result. From this we can express $p^*$ as:
\begin{align*}
p^* &= \inf_x f_0(x), \text{  x feasible}\\
&= \inf_x\sup_{\substack{\lambda\ge0 \\ \nu}}L(x, \lambda, \nu)
\end{align*}

and our dual is defined as
\begin{align*}
d^* = \sup_{\substack{\lambda\ge0 \\ \nu}}\inf_x L(x, \lambda, \nu)
\end{align*}

Hence strong duality can be expressed in the following way, clearly showing that $(x, \lambda, \nu)$ is a saddle point for $L$.
\begin{align*}
\inf_x\sup_{\substack{\lambda\ge0 \\ \nu}}L(x, \lambda, \nu) = \sup_{\substack{\lambda\ge0 \\ \nu}}\inf_x L(x, \lambda, \nu)
\end{align*}

\subsubsection{KKT Conditions}

We now state some conditions often used to determine whether a solution $x^*$ of $f$ is optimal. These are the \textbf{KKT Conditions}.

\begin{theorem}[KKT Conditions]
	Let $f_0, f_i, h_i$ be differentiable. If $x^*$ is optimal and $(\lambda^*, \nu^*)$ are dual optimal and we have strong duality; then the following Conditions must be satisfied:
	\begin{itemize}
		\item $\nabla_x f_0(x^*) + \sum_{i=1}^m \lambda_i^* \nabla_x f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla_x h_i(x^*) = 0$ $\rightarrow$ Stationarity
		\item $\lambda_i^* f_i(x^*)=0 \rightarrow$ Complementary Slackness
		\item $x^*$ is feasible $\rightarrow$ Primal Feasibility
		\item $\lambda^* \ge 0 \rightarrow$ Dual Feasibility
	\end{itemize}
\end{theorem}
\begin{proof}
	We have to show Stationarity and Complementary Slackness. Notice that
	\begin{align}
	f_{0}(x^*) = g(\lambda^*, \nu^*) &= \inf_x \left( f_0(x) + \sum_{i=1}^m \lambda_i^*f_i(x) + \sum_{i=1}^p \nu_i^* h_i(x) \right) \\
	&\le f_0(x^*) + \sum_{i=1}^m \lambda_i^*f_i(x^*) + \sum_{i=1}^p \nu_i^* h_i(x^*)\\
	&\overset{(a)}{\le} f_0(x^*)
	\end{align}
	Where (a) comes from the condition that $h_i(x^*) = 0 \Rightarrow \sum_{i=1}^p \nu_i^* h_i(x^*) = 0$ and $f_i(x^*) \le 0, \lambda_i \ge 0 \Rightarrow \sum_{i=1}^m \lambda_i^* f_i(x^*) \le 0$. We get that $\inf\limits_{x} L(x, \lambda^*, \nu^*) = f_0(x^*)$, i.e. $x^*$ minimizes $L(x, \lambda^*, \nu^*)$ which implies Stationarity. Complementary slackness comes from $\sum_{i=1}^m \lambda_i^* f_i(x^*) = 0$ and $\lambda_i^* f_i(x^*) \le 0 \Rightarrow \lambda_i^* f_i(x^*) = 0$ for every $i$.
\end{proof}

\begin{theorem}
	If $x^*, \lambda^*, \nu^*$ satisfy the KKT conditions and the primal is convex; then $x^*$ is optimal, $\lambda^*, \nu^*$ are dual optimal and we have strong duality.
\end{theorem}
\begin{proof}
		\begin{align}
		g(\lambda^*, \nu^*) &= \inf_x \left( f_0(x) + \sum_{i=1}^m \lambda_i^*f_i(x) + \sum_{i=1}^p \nu_i^* h_i(x) \right) \\
		&\overset{(a)}{=}f_0(x^*) + \sum_{i=1}^m \lambda_i^*f_i(x^*) + \sum_{i=1}^p \nu_i^* h_i(x^*)\\
		&\overset{(b)}{=}f_0(x^*)
		\end{align}
	 Since $\lambda^*\ge0$, $L(x,\lambda^*,\nu^*)$ is convex in $x$. By the KKT conditions, $x^*$ is primal feasible and $\nabla_x L(x,\lambda^*,\nu^*)$ evaluated at $x^*$ is 0 $\Rightarrow$ $x^*$ minimizes $L(x,\lambda^*,\nu^*)$ (a). (b) follows from complementary slackness. Therefore, we have strong duality and so $x^*$ is optimal and $\lambda^*, \nu^*$ are dual optimal.
\end{proof}

So the KKT conditions are necessary for optimality where $f_0,f_i,h_i$ are differentiable and there is strong duality. Likewise, if $L$ is convex and $x^*,\lambda^*,\nu^*$ satisfy KKT, then $x^*$ is optimal, $(\lambda^*,\nu^*)$ are dual optimal, and there is strong duality.

\begin{theorem}
	If Slaters condition is satisfied, then $x^*$ is optimal if and only if there exists $\lambda^*, \nu^*$ such that $(x^*,\lambda^*, \nu^*)$ satisfy KKT conditions.
\end{theorem}

\subsubsection{Using the Dual to solve the Primal}

If strong duality holds, $(\lambda^*, \nu^*)$ is a dual optimal solution and $L(x, \lambda^*, \nu^*)$ has a unique minimum value $x^*$ Then either:
\begin{enumerate}
	\item $x^*$ is feasible; then $x^*$ must be optimal.
	\item $x^*$ is not feasible and no optimal can exist.
\end{enumerate}

This means that in some cases maximizing the dual is equivalent to solving the primal.

\newpage

\section{Statistical Decision Theory}

\subsection{Formal Set-Up}

We need a general framework to make data-driven decisions under uncertainty. More formally, we observe some \textbf{Data} \(D \in \mathcal{D}\) which comes from some \textbf{Data Generating Distribution} \(D \sim P\). Let \(P\in \mathcal{P}\)\footnote{Often $P$ will describe an IID process, e.g. $D = (X_1,...,X_n)$ where $X_i \overset{iid}\sim P_0$. In this case, the loss is usually written w.r.t $P_0$ instead of $P$.}, where \(\mathcal{P}\) is the set of possible distributions. Let \(\mathcal{A}\) be our set of possible actions. To determine how good an action is, we define the \textbf{Loss} (cost) of doing that action as \(L: \mathcal{P} \times \mathcal{A} \mapsto \mathbb{R}\). The goal is to determine a \textbf{Decision Rule} \(\delta: \mathcal{D} \mapsto \mathcal{A}\) which, given data, produces an action.

Typically we consider \(\mathcal{P}\) as a Parametric Family of distributions, and we use \(\Theta\) interchangibly with \(\mathcal{P}\), using that \(P := P_{\theta}\).

\subsection{Procedure Analysis}

We need a way to assign a value to any \(\delta\) and a way to compare these values to find which one is ``best''. One such way of doing this is via the Frequentist Risk.

\subsubsection{Frequentist Risk Perspective} 
The first approach seeks to minimize the \textbf{Frequentist Risk}, which is defined as:
\begin{equation}\label{eq:1}
	R(P,\delta)=\mathbb{E}_{D\sim P}\{L(P,\delta(D))\}
\end{equation}

If we want to compare decision rules \(\delta_1, \delta_2\) using def \ref{eq:1}, we have to take into account \(P\), since \(R\) varies with both \(P\) and \(\delta\). Sometimes one decision rule \(\delta_1\) is better then another \(\delta_2\) regardless of \(P\), in which case we say \(\delta_1\) \textbf{Dominates} \(\delta_2\).
More formally:
$$R(P,\delta_1)\leq R(P,\delta_2) \forall P \in \mathcal{P}\ and$$
$$\exists P \in \mathcal{P},\ R(P,\delta_1) < R(P, \delta_2)$$

This basically means that \(\delta_1\) is a better decision rule then \(\delta_2\). Sometimes, there may be a ``best'' \(\delta\), one which isn't dominated by any other \(\delta_0\). We say \(\delta\) is \textbf{Admissible} if $\nexists \delta_0$ s.t. $\delta_0$ dominates $\delta$. Note: we should rule out inadmissible decision rules (except for simplicity or efficiency) but not necessarily accept Admissible ones!

Unfortunately, different \(P\)'s usually produce different optimal \(\delta\)'s! we must take into account the unknown \(P\) when minimizing (\ref{eq:1}). One way to take this into account is to use the \textbf{Minimax Criteria}: the optimal \(\delta\) minimizes the Frequentist Risk in worst case scenerio.

\begin{equation}\delta_{minimax} = \,\min\limits_{\delta}\,\max\limits_{P \in \mathcal{P}}\ R(P,\delta)
\end{equation}

If \(\mathcal{P}\) is a Parameteric Family, we can handle the dependence of \(R\) on \(P\) by averaging it out, adding weights \(\pi\) over $\Theta$ to put more weight on certain $\theta$'s. We can then minimize over \(\delta\). This is called the \textbf{Bayes Risk}, even though it is Frequentist concept since it averages over $D$ via (\ref{eq:1}).
\begin{equation}\label{eq:3}
\delta_{bayes}= \text{arg}\,\min\limits_{\delta}\,\int_{\Theta}^{}R(P_\theta,\delta)\pi(\theta)d\theta
\end{equation}

Where $\delta_{bayes}$ is called the \textbf{Bayes Rule}. Note that the Bayes Rule may not exist, and when they do they may not be unique.

\subsubsection{Bayesian Risk Perspective} 

Note that (\ref{eq:1}) does not consider that we only observed one \(D\). We can define a Risk function that does. The \textbf{Posterior Risk} is
\begin{equation}\label{eq:2}R_B(\delta|D) = \int_{\Theta}^{}L(P_{\theta},\delta)p(\theta|D)d\theta
\end{equation}
Where $p(\theta|D)$ is the posterior for a given prior $\pi(\theta)$. We can choose our decision rule based on this new risk function. This is called the \textbf{Bayes Estimator} or \textbf{Bayes Action} (not to be confused with the \textbf{Bayes Rule} above).

\begin{equation}\label{eq:5}
\delta_{post}= \text{arg}\,\min\limits_{\delta}\,R_B(\delta|D)
\end{equation}


Notice that in (\ref{eq:2}), we do not consider different unobserved values of \(D\), since the Bayesian would say they are irrelevent courtesy of the Conditionality Principle. For them, only the observed $D$ matters for inference. 
Additionally, \(\theta\) is integrated out in (\ref{eq:2}), meaning that (\ref{eq:5}) gives the undisputed optimal \(\delta\)! 

Note that the Frequentist can still use (\ref{eq:5}) by interpreting it as (\ref{eq:3}) with \(\pi\) as the ``true'' prior for \(\Theta\).
We would then get that:
\begin{align*}
 \int_{\Theta}^{}R(P_\theta,\delta)\pi(\theta)d\theta &= \int_{\Theta}^{}\int_{D}L(P_\theta,\delta)P(D|\theta)P(\theta)dDd\theta \\&\overset{(a)}{=} \int_{D}^{}\int_{\Theta}L(P_\theta,\delta)P(\theta|D)P(D)d\theta dD\\
&=\int_{D}^{}R_B(\delta|D)P(D)dD
\end{align*}

Where (a) is due to Fubini's theorem (provided the integral is finite).
It turns out that a \emph{Bayes rule} can be obtained by taking the \emph{Bayes action} for each particular $D$! See \cite{PHoffNotes2} for more details.

\newpage

\subsection{Types of Procedures}

\subsubsection{Parameter Estimation}\label{sec:parest}
Given a Parametric Family $\{P_{\theta}\}_{\theta\in\Theta}$, typically we have data \(D=(X^{(1)}, \cdots, X^{(n)})\) where each \(X^{(i)}\overset{iid}{\sim} P_{\theta}\). We want to use this data to estimate the true parameters \(\theta\). Hence, \(\mathcal{A} = \Theta\) and $\delta(D)$ is some an \textbf{Estimator} of \(\theta\). The estimator should minimize the loss (more specifically the risk). One popular loss function is the \textbf{Squared Loss}: \(L(\theta,\delta(D)) = ||\theta-\delta(D)||^2\). Note that since the data are IID we use the marginal density over $X$ instead of the joint over $D$ in the loss function.

If we take the expectation of the loss function above (the frequentist risk), we can decompose it nicely into two pieces:

\begin{align*}
R(P,\delta)&=\mathbb{E}_{D\sim P}\{||\theta-\delta(D)||^2\}\\
&=\mathbb{E}_{D\sim P}\{(\theta-\mathbb{E}_{D\sim P}\{\delta(D)\} + \mathbb{E}_{D\sim P}\{\delta(D)\} - \delta(D))^2\}\\
&=\underbrace{(\theta-\mathbb{E}_{D\sim P}\{\delta(D)\})^2}_{Bias^2} + \underbrace{\mathbb{E}_{D\sim P}\{(\mathbb{E}_{D\sim P}\{\delta(D)\} - \delta(D))^2\}}_{Variance}
\end{align*}

The above says that when we average this loss over all possible datasets, we can compare how much of the loss is due to the Bias and how much to the Variance of the estimator $\delta$. This idea works for other loss functions, but the decomposition is not nearly as clean. Finally, this is a Frequentist idea since it involves taking an expectation over the data generating distribution, an idea doesn't appeal to Bayesians since it is contrary to the conditionality principle. 
\\

We conclude by showing an interesting result: for parameter estimation, the bayes action for the squared loss
\(\delta_{post}(D) = \mathbb{E}\{\theta|D\}\). This is a simple optimization problem:
\begin{align*}
R_B(\delta|D) &= \int_{\Theta}^{}||\theta-\delta(D)||^2p(\theta|D)d\theta \\
&=\delta(D)^2 -2\delta(D)\int_{\Theta}^{}\theta p(\theta|D)d\theta+\int_{\Theta}^{}\theta^2 p(\theta|D)d\theta
\end{align*}

and taking the derivative and setting to 0 yields:
\begin{align*}
&\frac{\partial R_B}{\partial \delta} = 2\delta(D) -2\int_{\Theta}^{}\theta p(\theta|D)d\theta = 0 \\
&\Rightarrow \delta(D) = \int_{\Theta}^{}\theta p(\theta|D)d\theta = \mathbb{E}\{\theta|D\}
\end{align*}

\newpage

\subsubsection{Prediction}
Let $D=((X^{(1)},Y^{(1)}), \cdots, (X^{(n)},Y^{(n)}))$ where $X^{(i)} \in \mathcal{X}$ and $Y^{(i)} \in \mathcal{Y}$. 

We put a density on $X$ and $Y$: $(X^{(i)},Y^{(i)}) \overset{iid}{\sim} P_{XY}$. Our action space $\mathcal{A} = \mathcal{Y}^\mathcal{X}$, the set of functions $f: \mathcal{X}\mapsto \mathcal{Y}$. Hence \(\delta(D)\) is a \textbf{Learning Algorithm} which learns a function i.e. $\delta(D) = \hat{f}$. 

We can evaluate the performance of $f$ using a \emph{prediction loss} $l: \mathcal{Y} \times \mathcal{Y} \mapsto \mathbb{R}$, a measure of the distance between a given prediction and it’s associated ground truth. We define the \textbf{Generalization Error} from $l$ as:
\begin{equation}\label{eq:8}
L(P,f) = \mathbb{E}_{(X,Y)\sim P_{XY}}\{l(Y,f(X)) \}
\end{equation}
This is often called the Risk in Machine Learning. Note that we do not know (\ref{eq:8}), but we can approximate it using the below formula. This is called \textbf{Empirical Risk Minimization}
\begin{equation}\label{eq:erm}
L(P,f) = \frac{1}{n}\sum_{i=1}^n l\left(Y^{(i)},f(X^{(i)})\right)
\end{equation}

Notice that (\ref{eq:erm}) is just (\ref{eq:8}) with the empirical density substituted in place of the true, unknown one.

Prediction problems are called different things depending on whether $\mathcal{Y}$ is discrete or continuous. If $\mathcal{Y}$ discrete then the problem is called \textbf{Classification}, and if $\mathcal{Y}$ is not discrete e.g. $\mathcal{Y} = \mathbb{R}$, then it is referred to as \textbf{Regression}.

We previously described Prediction problems and noted that if $\mathcal{Y}$ is discrete then we refer to prediction as \textbf{Classification} and otherwise we refer to it as \textbf{Regression}. 

\subsubsection{Regression}
We look at Regression. Let $D=((X^{(1)},T^{(1)}), \cdots, (X^{(n)},T^{(n)}))$ with $X^{(i)} \in \mathcal{X}$ and $T^{(i)} \in \mathcal{T}$. Let $(X^{(i)},T^{(i)}) \overset{iid}{\sim} P_{XT}$ and $l(t, y(x)) = | t - y(x) |^2$ (the squared loss).
We need to find a function $y: \mathcal{X} \rightarrow \mathcal{T}$ which minimizes the Generalization Error:
\begin{align*}
L(P,y) &= \mathbb{E}_{(X,T)\sim P_{XT}}\{| t - y(x) |^2\}\\
&= \int \int | t - y(x) |^2 p(x, t) dx dt
\end{align*}

In this case we can find the optimal $y$ by using calculus of variations. That is to say, the problem is reduced to an optimization problem. We denote $G(y, y', x) = \int | t - y(x) |^2 p(x, t) dt$ and use the Euler Lagrange equations to get that our stationary point must occur at 
\begin{align*}
&\frac{\partial G(y, y', x)}{\partial y} - \frac{d}{dx}\frac{\partial G(y, y', x)}{\partial y'} = 0\\
&\Rightarrow \frac{\partial G(y, y', x)}{\partial y}=0
\end{align*}

Since $\frac{\partial G(y, y', x)}{\partial y'}=0$ since $y'$ is not in $G$. We then solve:

\begin{align*}
\frac{\partial L(P,y)}{\partial y(x)} &= \frac{\partial}{\partial y(x)}\int | t - y(x) |^2 p(x, t) dt\\
&=2 \int (t - y(x)) p(x, t) dt = 0\\
\end{align*}

Solving for the above we have that

\begin{align*}
\int (t - y(x)) p(x, t) dt &= \int t p(x, t) dt - \int y(x) p(x, t) dt = 0\\
&\Rightarrow \int t p(x, t) dt =  y(x)p(x) \\
&\Rightarrow y(x) = \int t \frac{p(x, t)}{p(x)} dt = \mathbb{E}_{t \sim p(t|x)}\{t|x\}
\end{align*}

And so our learning algorithm $\delta(D)$ simply returns $y(x) = \mathbb{E}_{t \sim p(t|x)}\{t|x\}$. Of course, we don't know $p(t|x)$, so we would have to approximate it using the Empirical Risk Minimization described before!


The squared loss functions is a member of loss functions called the \textbf{Minkowski Loss}. This family has the following form:
\begin{equation}
L_q(P,y) = \int\int |t-y(x)|^q p(x,t)dxdt
\end{equation}

We solve for the optimal $y(x)$ and set this to 0:
\begin{align}
\frac{\partial L_q(P,y)}{\partial y(x)} &= \int q|t-y(x)|^{q-1} sgn(t-y(x)) p(x,t)dt\\
&= \int_{y(x)}^{\infty} q|t-y(x)|^{q-1}p(x,t)dt-\int_{-\infty}^{y(x)} q|t-y(x)|^{q-1}p(x,t)dt\\
&\Rightarrow \int_{-\infty}^{y(x)} |t-y(x)|^{q-1}p(x,t)dt = \int_{y(x)}^{\infty} |t-y(x)|^{q-1}p(x,t)dt
\end{align}

For $q=1$, we see that $y(x)$ is the conditional median of $t$.

\begin{align}
\int_{-\infty}^{y(x)} p(x,t)dt = \int_{y(x)}^{\infty} p(x,t)dt
\end{align}

Finally, as $q \rightarrow 0$, the $y(x)$ given by the Minkowski loss is the conditional mode of $t$.

\newpage

\section{Information Theory}

We want a function $I$ which measures how much information you learn from observing some event $E$. We want it to satisfy some properties, mainly:

\begin{enumerate}
	\item Highly probable $E$ have low $I(E)$ and conversely $\rightarrow$ \emph{ rare events give more information}.
	\item $I(E) \ge 0$ $\rightarrow$\emph{ Information is non-negative}.
	\item if $p(E)=1$ then $I(E) = 0$ $\rightarrow$\emph{ Events that always occur provide no information}.
	\item If $E_1, E_2$ are independent events then $I(E_1 \cap E_2) = I(E_1) + I(E_2)$ $\rightarrow$\emph{ information due to independent events are additive}.
\end{enumerate}
From 1. and 3. we see that $I$ should be a function of the probability of an events occurence, i.e. $I(E)=f(p(E))$ for some $f$. From 4., given independent events $E_1, E_2$, we have that:
\begin{align}
f(p(E_1)p(E_2)) &= f(p(E_1\cap E_2)) = f(p(E_1)) + f(p(E_2))\\
f(x\cdot y) &= f(x) + f(y)
\end{align}

If we assume that $I$ is continuous, then only $I(E) = Klog p(E)$ satisfies (11)  \cite{EntNotes}. Finally, using 2., we see that $K<0$. We can then define $I$ as:

\begin{equation}
I(E) = -log p(E)
\end{equation}

Where the choice of $K$ decides the base of the logarithm. In this case we set it to $1$ for clarity.

\subsection{Entropy}
We can extend this notion to a discrete Random Variable $X\sim p$ with finite domain $\mathcal{X}$. By defining the \textbf{Shannon Entropy} $H(X)$ as the average amount of information i.e. 
\begin{equation}
H(X) = \mathbb{E}_{X\sim p}\{I(X)\} = \mathbb{E}_{X\sim p}\{-\log p(X)\} = -\sum_{x\in\mathcal{X}}p(x)\log p(x)
\end{equation}
We can also denote this as $H(p)$ where $p \sim X$ depending on what we want to emphasize. Note that WLOG we can assume that \(p(x)>0 \ \forall x\in\mathcal{X}\). This is because we can use the convention that \(0\cdot log0 = 0\) (based on continuity arguments). Hence zero probability outcomes do not contribute to $H(X)$ anyways. We can further extend this for two Random Variables $X$, $Y$ with finite domain \(\mathcal{X}\times\mathcal{Y}\) by defining the \textbf{Joint Entropy} as:
\begin{equation}
H(X,Y) = -\sum_{x,y}p_{XY}(x,y)\log p_{XY}(x,y)
\end{equation}

The \textbf{Conditional Entropy} is defined as:
\begin{equation}
H(X|Y) = \mathbb{E}_{X|Y}\{-\log p(X|Y)\} = -\sum_{x,y}p_{XY}(x,y)\log p_{X|Y}(x|y)
\end{equation}

These quantities have nice properties:
\begin{enumerate}
	\item \emph{Non-negativity}: \(H(X)\ge0\), with equality only when X is a constant.
	
	PROOF: WLOG we assume that \(p(x)>0 \ \forall x\in\mathcal{X}\). We have that \(H(X) = -\sum_{x} p(x)logp(x) = \sum_{x} p(x)logp(x)^{-1}\ge0\), since \(p(x)>0\) and \(p(x)^{-1} \ge 1\). If \(H(X)=0\) then \(\exists \alpha\) such that \(p(\alpha)^{-1}=1 \Rightarrow p(\alpha)=1\). Hence \(X\) must be a constant, as needed.
	
	\item \emph{Chain Rule}: $H(X,Y) = H(X \mid Y) + H(Y) = H(Y \mid X) + H(X)$ 
	\item \emph{Monotonicity}: $H(X\mid Y) \le H(X)$ 
\end{enumerate}

\subsection{KL Divergence}
We can now look at the \textbf{KL Divergence} or \textbf{Relative Entropy}. This quantity measures the ``distance'' between two probability mass functions $p$ and $q$.

\begin{equation}
KL(p||q) = \mathbb{E}_{X\sim p}\left\{\log \frac{p(X)}{q(X)}\right\} = \sum_{x\in\mathcal{X}}p(x)\log\frac{p(x)}{q(x)}
\end{equation}

The KL divergence has some nice properties.
\begin{enumerate}
	\item $KL(p||q) \ge 0$ with equality iff $p=q$
	
	PROOF: If there exists $x \in \mathcal{X}$ such that $p(x) = 0$ and $q(x) > 0$, then $KL(p || q) = \infty$.	Otherwise:
	\begin{align*}
	-KL(p||q) &= \mathbb{E}_{X\sim p}\left\{\log \frac{q(X)}{p(X)}\right\}\\
	&\overset{(a)}{\le} \log \mathbb{E}_{X\sim p}\left\{\frac{q(X)}{p(X)}\right\} \\
	&= \log \sum_{x} p(x)\frac{q(x)}{p(x)} = \log \sum_{x} q(x) = 0
	\end{align*}
	Where (a) follows from Jensen's inequality. $KL(p||q) = 0$ only occurs when there is equality in Jensen's inequality, which only occurs when $p(x)=cq(x)$ for some $c$. Since $\sum_{x}cq(x) = c\sum_{x}q(x) = c \Rightarrow c=1$, so $p=q$ as needed.
	
	\item $KL(p||q)$ is strictly convex in each argument
	
	\item $KL(p||q) \ne KL(q||p)$ so it is not a metric
	
	\item We can decompose the KL divergence into two seperate terms:
	
	\begin{align} KL(p||q) &= \sum_{x\in\mathcal{X}}p(x)\log p(x) - \sum_{x\in\mathcal{X}}p(x)\log q(x) \\
	&= -H(p) + \mathbb{E}_{X\sim p}\{-\log q(x)\} \\
	&= -H(p) + CE(p,q)
	\end{align}
	
	Where the $H(p)$ is the Entropy and $CE(p,q)$ is called the \textbf{Cross Entropy}.
	
\end{enumerate}

\subsection{Mutual Information}
We can quantify the amount of information obtained about one discrete random variable $X$, through another $Y$ by defining the \textbf{Mutual Information} as:
\begin{equation}
I(X,Y)=\sum_{x,y}p_{X,Y}(x,y)\log\frac{p_{XY}(x,y)}{p_X(x)p_Y(y)}
\end{equation}

We again assume WLOG that \(p(x,y)>0 \ \forall (x,y)\in\mathcal{X}\times\mathcal{Y}\). We note the following properties of $I$:

\begin{enumerate}
	\item \(I(X,X) = H(X)\) \(\rightarrow\) Sometimes the Entropy is called the \textbf{Self Information}
	\item $I(X,Y) = KL(p_{XY}||p_Xp_Y)$
	\item \(I(X,Y)\ge 0\)
	\begin{proof}
		Notice that \(I(X,Y) = KL(p_{XY}||p_Xp_Y) \ge 0\) by the positiveness of \(KL(\cdot||\cdot)\)
	\end{proof}
	\item $I(X,Y) = H(p_X) + H(p_Y) -H(p_{XY})$
	\begin{proof} We use property 2. of $I$ and property 4. of \(KL(\cdot||\cdot)\)
		\begin{align*}
		I(X,Y) &= KL(p_{XY}||p_Xp_Y) \\
		&= -H(p_{XY}) + CE(p_{XY},p_Xp_Y) \\
		&= -H(p_{XY}) - \sum_{x,y}p_{X,Y}(x,y)\log p_X(x)p_Y(y)\\
		&= -H(p_{XY}) - \left(\sum_{x,y}p_{X,Y}(x,y)\log p_X(x) + \sum_{x,y}p_{X,Y}(x,y)\log p_Y(y)\right)\\
		&=-H(p_{XY}) - \left(\sum_{x}p_{X}(x)\log p_X(x) + \sum_{y}p_{Y}(y)\log p_Y(y)\right)\\
		&= -H(p_{XY}) + H(p_X) + H(p_Y)
		\end{align*}
	\end{proof}
	
\end{enumerate}

\subsection{Differential Entropy}
We can define the Entropy, KL divergence and Mutual Information for continuous random variables.

\begin{align}
H(p) &= -\int_{x\in\mathcal{X}}p(x)\log p(x)dx \\
KL(p,q) &= \mathbb{E}_{X\sim p}\left\{\log \frac{p(X)}{q(X)}\right\} = \int p(x)\log\frac{p(x)}{q(x)}dx\\
I(X,Y) &=\int\int p_{X,Y}(x,y)\log\frac{p_{XY}(x,y)}{p_X(x)p_Y(y)}dxdy
\end{align}

In the continuous case, the properties previously described hold except that the entropy is no longer necessarily non negative. For an example of this let $p = Uniform\left(\frac{1}{2},1\right)$. Then $H(p) = log(\frac{1}{2}) < 0$.

\subsection{Entropy and Estimation}

The KL divergence can be used within the Decision Theoretic framework. Semantically, $KL(p||q)$ represents how well some distribution $q$ approximates the ``true'' $p$. Suppose we wanted to estimate a distribution $p$ which we knew belonged to a Parametric Family  $p\in \{p_{\theta}\}_{\theta\in\Theta}$. Let \(\mathcal{A} = \{p_{\theta}\}_{\theta\in\Theta}\) and $\delta(D)=p_{\theta}$. Recall that this is similar to the Estimation problem described in \ref{sec:parest} \footnote{We modify the problem to make explicit the intention of estimating the density rather then the parameter. These goals are the same provided the parametric family is \textbf{identifiable}}.  We can define the \textbf{Negative Log Loss}:
\begin{equation}
L(p,p_{\theta}) = -\log p_{\theta}(X)
\end{equation}

This loss makes sense as $p_{\theta}(X)$ small means that the model has not taken into account $X$, and the corresponding loss will be large. The Cross Entropy is the corresponding risk function for this:
\begin{equation}\label{eq:celf}
R(p,p_{\theta}) = \mathbb{E}_{X\sim p}\{ -\log p_{\theta}(X) \}
\end{equation}	

This risk also makes sense. $KL(p,p_{\theta}) = -H(p) + L(p,p_{\theta})$, and since $H(p)$ is constant, minimizing the KL is equivalent to minimizing the cross entropy. Since $KL(p,p_{\theta}) \ge 0$ we see that the minimum is attained at $L(p,p_{\theta}) = H(p)$, which occurs when $p_{\theta}=p$ i.e. when our prediction matches the ``true'' density.

\newpage

\subsubsection{Maximum Likelihood Estimation}

We don't know $p$, so we cannot compute (\ref{eq:celf}). Instead, we can use in its place the empirical density function $\hat{p}$, as defined in \ref{sec:empdist}. Given $X$ discrete, it turns out that the MLE for $\theta$ is the same as $\text{arg}\,\min\limits_{\theta\in\Theta} KL(\hat{p}||p_{\theta})$. This is because:

\begin{align*}
KL(\hat{p}||p_{\theta}) &= -H(\hat{p}) + CE(\hat{p},p_{\theta}) \\
&= -H(\hat{p}) - \sum_{x\in\mathcal{X}}\hat{p}(x)\log p_{\theta}(x)\\
&= -H(\hat{p}) - \frac{1}{n}\sum_{x\in\mathcal{X}}\sum_{i=1}^n\delta(x,x^{(i)}) \log p_{\theta}(x)\\
&= -H(\hat{p}) - \frac{1}{n}\sum_{i=1}^n\log p_{\theta}(x^{(i)})\\
&= -H(\hat{p}) - \frac{1}{n}l(\theta \mid x^{(1)}, \cdots, x^{(n)})
\end{align*}

This provides a nice interpretation for the MLE - it is finding the $p\in \{p_{\theta}\}_{\theta\in\Theta}$ which minimizes the dissimilarity between the empirical distribution of the training set and itself as measured by the KL divergence. Conversly we can justify the use of the Cross Entropy loss through its equivalence to Maximum Likelihood. Note that this holds for $X$ continuous, we just have to change the sums for integrals.

On a final note, one may think that the quantity $KL(p_{\theta}|| \hat{p})$ could be interesting. They would be wrong. This is since $p_{\theta}(x)=0 \Rightarrow \hat{p}_{\theta}(x)=0$ but $\hat{p}_{\theta}(x)=0 \not\Rightarrow p_{\theta}(x)=0$ since $\hat{p}_{\theta}(x)=0$ only means that the particular value of $x$ wasn't observed in the sample.	


\subsubsection{Maximum Entropy Principle}

The \textbf{Principle of Maximum Entropy} (MaxENT) states that the probability distribution which best represents the ``current state of knowledge" is the one with the largest entropy. More specifically, given some subset of distributions on $\mathcal{X}$ denoted as $\mathcal{M}$, we want to chose as our estimated distribution:
$$\arg\max\limits_{q \in \mathcal{M}} H(q)$$

We may impose constraints to this in the form of \textbf{Testible Information}-- statements about $q$ with well-defined truth or falsity. The most basic of these is that $\int_{\mathcal{X}}^{} q(x)dx = 1$. We now show a few maximum entropy distributions. 

\newpage

\begin{theorem}\label{th:unifentlim}
Let $X\sim p$ be a RV with finite support $\mathcal{X}$, $|\mathcal{X}|=k$, and $\mathcal{M} = \Delta_{k}$. The uniform density is the MaxENT density.
\end{theorem}

\begin{proof}
	We derive the following upper bound for \(H(p)\)
	\begin{equation}
	H(p) \le \log k
	\end{equation}
	
	To derive this inequality, let \(q\sim Uniform\) on $\mathcal{X}$. We have that:
	\begin{align*}
	D(p||q)&=\sum_{x}p(x)\log\frac{p(x)}{q(x)}\\
	&=\sum_{x}p(x)\log p(x)-\sum_xp(x)\log q(x)\\
	&= -H(p) + \sum_xp(x)\log k \\
	&= -H(p) + \log k
	\end{align*}
	
	and so \(H(p)=\log k-D(p||q) \Rightarrow H(p) \le \log k \) as needed. Since $H(q) = \log k$ we can see that equality holds iff $p \sim Uniform$.
\end{proof}

So we have that, for densities with finite support and no testible information (apart from being a valid pmf), the MaxENT solution is uniform.


\begin{theorem}
	The MaxENT density for Random Variables $X_1 \in \mathcal{X}_1$ and $X_2 \in \mathcal{X}_2$ with $X_1 \sim p_1$ and $X_2 \sim p_2$ is \( (X_1,X_2)\sim p_1p_2 \). i.e. higher entropy assumes independence.
\end{theorem}

\begin{proof}
Properties 3. and 4. of $I$ gives us that \(I(X_1,X_2)\ge0 \Rightarrow H(X_1) + H(X_2) \ge H(X_1,X_2) \), and so the maximal entropy of \((X_1,X_2)\) is \(H(X_1) + H(X_2)\). By definition this only occurs when \(I(X_1,X_2)=0\), which only occurs if
\(p_{1,2}(x_1,x_2)=p_1(x_1)p_2(x_2) \ \forall x_1,x_2 \in \mathcal{X}_1\times\mathcal{X}_2\).
\end{proof}

\begin{theorem}
The MaxENT of $X$ with $\mathcal{X} = \mathbb{N}$ and with testible information $E(X)=\alpha$ is the Geometric Distribution $p(k)=\left(\frac{\alpha}{1+\alpha}\right)^k \frac{1}{1+\alpha}$
\end{theorem}

\begin{proof}
We want to find the distribution which maximizes the entropy $H(p)$ satisfying the constraints $\mathbb{E}(X)=\alpha$ and $\sum_{i=0}^{\infty}p(i)=1$. We form the Lagrangian:

\begin{align*}
L(p,\nu, C)=-H(p)+\nu \left(\sum_{i=0}^{\infty} ip(i)-\alpha\right) + C\left( \sum_{i=0}^{\infty}p(i)-1 \right)
\end{align*}

Taking the derivative w.r.t. $p(k)$ we get:
\begin{align}\label{eq:p}
\frac{\partial}{\partial p(k)}L(p,\nu, C)&= -\log p(k) - 1 + k\nu + C\\
&\Rightarrow p(k) = \exp\{k\nu\}\exp\{C-1\}
\end{align}
And using that $\sum_{i=0}^{\infty}p(i)=1$ we have that

\begin{align}\label{eq:cterm}
\sum_{i=0}^{\infty}\exp\{i\nu\}\exp\{C-1\}
= 1 \Rightarrow \exp\{-C+1\} = \sum_{i=0}^{\infty}\exp\{i\nu\}
\end{align}

we substitute (\ref{eq:cterm}) into (\ref{eq:p}) to eliminate $C$

\begin{align}\label{eq:pnoc}
p(k)=\frac{\exp\{k\nu\}}{\sum_{i=0}^{\infty}\exp\{i\nu\}}
\end{align}

We then solve for $\alpha$

\begin{align*}
\mathbb{E}(X)&=\sum_{k=0}^{\infty}\frac{k\exp\{k\nu\}}{\sum_{i=0}^{\infty}\exp\{i\nu\}}=\alpha\\
&\Rightarrow \sum_{k=0}^{\infty}k\exp\{k\nu\}=\alpha\sum_{i=0}^{\infty}\exp\{i\nu\}\\
&\overset{(a)}{\Rightarrow} \frac{\exp\{\nu\}}{(1-\exp\{\nu\})^2} = \frac{\alpha}{(1-\exp\{\nu\})}\\
&\Rightarrow \exp\{\nu\} = \frac{\alpha}{1+\alpha}
\end{align*}

Where (a) comes from the geometric series. Finally, we sub this value into (\ref{eq:pnoc}) to get the familiar formula:

\begin{align}
p(k)=\left(\frac{\alpha}{1+\alpha}\right)^k\frac{1}{1+\alpha}
\end{align}
\end{proof}

\newpage

\subsubsection{MaxENT and the Exponential Family}

It turns out that if the only testible information we have about our pdf are moment constraints, then the MaxENT solution always belongs to the Exponential Family from \ref{sec:expfam}.

\begin{theorem}
	If $X_1, \cdots, X_n$ are an IID sample and $T_1(X), \cdots, T_d(X)$ are statistics, then the MaxENT estimator satisfying $\mathbb{E}_{q}\{T_j(X)\} = \mathbb{E}_{\hat{p}}\{T_j(X)\} \ j = 1,\cdots,d$ is the MLE distribution in the exponential family with sufficient statistics $T(X)$
\end{theorem}

\begin{proof}
For simplicity let $X$ be finite with $\mathcal{X}$ and $k$ as defined before. Suppose we have statistics $T_1(X), \cdots, T_d(X)$ and we define $\mathcal{M}$ as
\begin{equation}
\mathcal{M} = \Bigl\{ q: \underset{\substack{model \ expected \\ feature \ count}}{\mathbb{E}_{q}\{T_j(X)\}} = \underset{\substack{empirical \\ feature \ count}}{\mathbb{E}_{\hat{p}}\{T_j(X)\}} \ j = 1,\cdots,d \Bigr\}
\end{equation}

Our testible information are the $d$ \emph{moment constraints}. Using the relation \(H(p)=\log k-D(p||q)\) derived from theorem \ref{th:unifentlim} we have the following alternative characterization of MaxENT:
\begin{align}
\arg\max\limits_{q \in \mathcal{M}} H(q) = \arg\min\limits_{q \in \mathcal{M}} KL(q, Uniform)
\end{align}
We then pose the MaxENT problem as an optimization problem from \ref{sec:optim} 
\begin{align*}
\text{Minimize } & \sum_{x}q(x)\log \frac{q(x)}{u(x)} \\
\text{subject to } & q(x)\ge 0 \\
& \sum_x q(x) = 1 \\
& \sum_x q(x)T_j(x) = \alpha_j \ j = 1, \cdots, d
\end{align*}

Where $u(x) = \frac{1}{k} \ \forall x\in \mathcal{X}$. Our Lagrangian is:
\begin{align*}
L(q, \lambda, \nu) &= \sum_{x}q(x)\log \frac{q(x)}{u(x)} + \sum_{j=1}^d \lambda_j \left( \alpha_j - \sum_x q(x)T_j(x) \right) + \nu\left(1 - \sum_x q(x) \right)\\
&= \mathbb{E}_{q}\left\{ \log \frac{q(x)}{u(x)} \right\} + \alpha^T\lambda - \mathbb{E}_{q}\left\{\lambda^TT(x)\right\} + \nu - \mathbb{E}_{q}\{\nu\}
\end{align*}

We find the dual function (\ref{eq:dual}). First we find the $q$ which minimizes $L$:
\begin{align*}
\frac{\partial L(q| \lambda, \nu)}{\partial q(x)} &= 1 + \log \frac{q(x)}{u(x)} -  \lambda^TT(x) - \nu = 0\\
&\Rightarrow q^*(x| \nu, \lambda) = u(x)\exp\{ \lambda^TT(x) + \nu -1\}
\end{align*}
We then compute the dual:
\begin{align*}
g(\lambda, \nu) &=\min\limits_{q\in \mathcal{M}} L(q^*(x| \nu, \lambda),\lambda,\nu)\\
&= L(q^*(x| \nu, \lambda),\lambda,\nu)\\
&= \mathbb{E}_{q^*}\left\{ \lambda^TT(x) + \nu -1 \right\} +  \alpha^T\lambda - \mathbb{E}_{q*}\left\{\lambda^TT(x)\right\} + \nu - \mathbb{E}_{q^*}\{\nu\}\\
&=\alpha^T\lambda + \nu - \mathbb{E}_{q^*}\left\{1\right\}\\
&=\alpha^T\lambda + \nu - \underbrace{\sum_x u(x)\exp\{ \lambda^TT(x)\}}_{=Z(\lambda)}e^{\nu -1}
\end{align*}

From Slaters condition and the convexity of $L$, if $\exists q \in \mathcal{M}$ s.t. $q(x)>0 \ \forall x$ we get strong duality. We can assume WLOG that such a $q$ exists since, if it didn't, we could just restrict our domain to $\mathcal{X} \setminus \{ x | q(x) = 0 \}$. We solve the dual problem. We first maximize w.r.t $\nu$
\begin{align*}
\frac{\partial  g(\lambda, \nu)}{\partial \nu } &= 1 - Z(\lambda)e^{\nu -1} = 0\\
&\Rightarrow  e^{\nu^*-1} = \frac{1}{Z(\lambda)}
\end{align*}

And we substitute our optimum $\nu^*$:
\begin{align*}
\max\limits_{\nu \in \mathbb{R}} L(q^*(x| \nu, \lambda),\lambda,\nu) &= L(q^*(x| \nu^*, \lambda),\lambda,\nu^*)\\
&= \alpha^T\lambda + \nu - \underbrace{Z(\lambda)e^{\nu^* -1}}_{=1}\\
&= \alpha^T\lambda + \underbrace{\nu - 1}_{-\log Z(\lambda)}
\end{align*}

Finally, we use that $\alpha_j = \mathbb{E}_{\hat{p}}\{T_j(X)\}$
\begin{align*}
L(q^*(x| \nu^*, \lambda),\lambda,\nu^*) &= \alpha^T\lambda -\log Z(\lambda)\\
&= \mathbb{E}_{\hat{p}}\{T(X)\}^T\lambda - \log Z(\lambda)\\
&= \frac{1}{n}\sum_{i=1}^n \left( T(X_i)^T\lambda - \log Z(\lambda) \right)
\end{align*}

Let  $p(X_i | \lambda) = q^*(X_i \mid \nu^*, \lambda) = u(x)\exp\{ \lambda^TT(x) - \log Z(\lambda)\}$. This is a pdf belonging to the Exponential family. We see the correspondence between maximizing the dual and maximum likelihood on the Exponential family since:
\begin{align*}
L(q^*,\lambda,\nu^*) &\propto \frac{1}{n}\sum_{i=1}^n \Bigl( \log p(X_i \mid \lambda) \Bigr) \\
&=\frac{1}{n}\mathcal{L}(X_1, \cdots, X_n | \lambda)
\end{align*}
\end{proof}

Supposing we solved the MLE problem above, we then have that 
$$q^*(x)=u(x)\exp\{ (\lambda^*)^TT(x) - \log Z(\lambda^*)\}$$
Since we have strong duality and $(\lambda^*, \nu^*)$ are dual optimal and $q^*$ is primal feasible (by construction it satisfies all of the constraints), it must be optimal.

\newpage

\bibliography{Note1.bib}
\bibliographystyle{ieeetr}

\end{document}